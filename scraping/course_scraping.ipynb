{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Setup\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344947c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configure API & Constants\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "BASE_URL = \"https://catalog.unc.edu\"\n",
    "COURSE_INDEX_URL = f\"{BASE_URL}/courses/#text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2c1151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: RequisiteParser Class\n",
    "class RequisiteParser:\n",
    "    def __init__(self, delay: float = 0.2):\n",
    "        self.model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "        self.delay = delay\n",
    "        self.api_calls = 0\n",
    "        self.failed_parses = []\n",
    "\n",
    "    def parse_requisites(self, raw: str, course_id: str = None) -> dict:\n",
    "        if not raw or not raw.strip():\n",
    "            return {\n",
    "                \"prerequisites\": [],\n",
    "                \"corequisites\": [],\n",
    "                \"grade_requirements\": {},\n",
    "                \"requisites_note\": None\n",
    "            }\n",
    "        if self.api_calls > 0:\n",
    "            time.sleep(self.delay)\n",
    "        self.api_calls += 1\n",
    "\n",
    "        prompt = f\"\"\"Parse the following course requisite statement and return a JSON object with this exact structure:\n",
    "\n",
    "{{\n",
    "    \"prerequisites\": [ /* AND-groups of OR-alternatives */ ],\n",
    "    \"corequisites\": [ /* same */ ],\n",
    "    \"grade_requirements\": {{ /* course â†’ grade */ }},\n",
    "    \"requisites_note\": null\n",
    "}}\n",
    "\n",
    "Course Requisite:\n",
    "{raw}\n",
    "\n",
    "Return ONLY the JSON.\"\"\"\n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            json_text = response.text.strip()\n",
    "            json_text = re.sub(r'^```json\\s*', '', json_text)\n",
    "            json_text = re.sub(r'\\s*```$', '', json_text)\n",
    "            result = json.loads(json_text)\n",
    "            return self._validate_result(result)\n",
    "        except Exception as e:\n",
    "            if course_id:\n",
    "                self.failed_parses.append((course_id, str(e)))\n",
    "            return self._fallback_parse(raw)\n",
    "\n",
    "    def _validate_result(self, result: dict) -> dict:\n",
    "        validated = {\n",
    "            \"prerequisites\": result.get(\"prerequisites\", []),\n",
    "            \"corequisites\": result.get(\"corequisites\", []),\n",
    "            \"grade_requirements\": result.get(\"grade_requirements\", {}),\n",
    "            \"requisites_note\": result.get(\"requisites_note\", None)\n",
    "        }\n",
    "        for key in [\"prerequisites\", \"corequisites\"]:\n",
    "            lst = validated[key]\n",
    "            if not isinstance(lst, list):\n",
    "                validated[key] = []\n",
    "            else:\n",
    "                cleaned = []\n",
    "                for item in lst:\n",
    "                    if isinstance(item, list):\n",
    "                        cleaned.append(item)\n",
    "                    elif isinstance(item, str):\n",
    "                        cleaned.append([item])\n",
    "                validated[key] = cleaned\n",
    "        if not isinstance(validated[\"grade_requirements\"], dict):\n",
    "            validated[\"grade_requirements\"] = {}\n",
    "        return validated\n",
    "\n",
    "    def _fallback_parse(self, raw: str) -> dict:\n",
    "        COURSE_RE = re.compile(r'\\b[A-Z]{2,5}\\s?\\d{2,3}[A-Z]?\\d?\\b')\n",
    "        courses = COURSE_RE.findall(raw)\n",
    "        normalized = []\n",
    "        for c in courses:\n",
    "            if ' ' not in c:\n",
    "                c = re.sub(r'([A-Z]+)(\\d)', r'\\1 \\2', c)\n",
    "            normalized.append(c)\n",
    "        prereqs = [[c] for c in normalized]\n",
    "        grades = {}\n",
    "        if 'C or better' in raw or 'grade of C' in raw:\n",
    "            for c in normalized:\n",
    "                grades[c] = 'C'\n",
    "        note = None\n",
    "        if 'permission' in raw.lower() or 'instructor' in raw.lower():\n",
    "            note = \"Permission of instructor may be required\"\n",
    "        return {\n",
    "            \"prerequisites\": prereqs,\n",
    "            \"corequisites\": [],\n",
    "            \"grade_requirements\": grades,\n",
    "            \"requisites_note\": note\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ac923bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Scraping Functions\n",
    "def get_department_links(only=None):\n",
    "    response = requests.get(COURSE_INDEX_URL)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    index_div = soup.find(\"div\", {\"id\": \"atozindex\"})\n",
    "    links = []\n",
    "    for a in index_div.find_all(\"a\", href=True):\n",
    "        dept = a['href'].split(\"/\")[-2].upper()\n",
    "        if only is None or dept in only:\n",
    "            links.append((dept, urljoin(BASE_URL, a['href'])))\n",
    "    return links\n",
    "\n",
    "def parse_course_block(block, parser: RequisiteParser):\n",
    "    data = {\n",
    "        \"department\": None,\n",
    "        \"course_number\": None,\n",
    "        \"course_id\": None,\n",
    "        \"course_name\": None,\n",
    "        \"credits\": None,\n",
    "        \"description\": None,\n",
    "        \"requisites\": {\"prerequisites\": [], \"corequisites\": []},\n",
    "        \"grade_requirements\": {},\n",
    "        \"requisites_note\": None,\n",
    "        \"gen_ed\": None,\n",
    "        \"grading_status\": None\n",
    "    }\n",
    "    header = block.find(\"div\", class_=\"cols noindent\")\n",
    "    if header:\n",
    "        strongs = header.find_all(\"strong\")\n",
    "        if len(strongs) >= 3:\n",
    "            code = strongs[0].text.strip()\n",
    "            if \" \" in code:\n",
    "                dept, num = code.split(\" \", 1)\n",
    "                num = num.rstrip(\".\")\n",
    "                data[\"department\"], data[\"course_number\"] = dept, num\n",
    "                data[\"course_id\"] = f\"{dept} {num}\"\n",
    "            data[\"course_name\"] = strongs[1].text.strip()\n",
    "            data[\"credits\"] = strongs[2].text.strip().replace(\" Credits.\", \"\")\n",
    "    desc = block.find(\"p\", class_=\"courseblockextra\")\n",
    "    if desc:\n",
    "        data[\"description\"] = desc.text.strip()\n",
    "    req = block.find(\"span\", class_=\"text detail-requisites margin--default\")\n",
    "    if req:\n",
    "        cid = data.get(\"course_id\", None)\n",
    "        parsed = parser.parse_requisites(req.text, cid)\n",
    "        data[\"requisites\"] = {\n",
    "            \"prerequisites\": parsed[\"prerequisites\"],\n",
    "            \"corequisites\": parsed[\"corequisites\"]\n",
    "        }\n",
    "        data[\"grade_requirements\"] = parsed[\"grade_requirements\"]\n",
    "        data[\"requisites_note\"] = parsed[\"requisites_note\"]\n",
    "    idea = block.find(\"span\", class_=\"text detail-idea_action margin--default\")\n",
    "    if idea:\n",
    "        data[\"gen_ed\"] = idea.text.strip().replace(\"IDEAs in Action Gen Ed:\", \"\")\n",
    "    grade = block.find(\"span\", class_=\"text detail-grading_status margin--default\")\n",
    "    if grade:\n",
    "        data[\"grading_status\"] = grade.text.strip().replace(\"Grading Status: \", \"\")\n",
    "    return data\n",
    "\n",
    "def parse_department(url, parser: RequisiteParser, dept_code: str = None):\n",
    "    \"\"\"Parse all courses from a department page.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    course_blocks = soup.find_all(\"div\", class_=\"courseblock\")\n",
    "    \n",
    "    total_courses = len(course_blocks)\n",
    "    print(f\"   Found {total_courses} courses to parse\")\n",
    "    \n",
    "    courses = []\n",
    "    for i, cb in enumerate(course_blocks, 1):\n",
    "        # Extract course ID for progress display\n",
    "        header = cb.find(\"div\", class_=\"cols noindent\")\n",
    "        course_id = \"Unknown\"\n",
    "        if header:\n",
    "            strong_tags = header.find_all(\"strong\")\n",
    "            if strong_tags:\n",
    "                course_id = strong_tags[0].text.strip()\n",
    "        \n",
    "        print(f\"   Processing {course_id} ({i}/{total_courses})...\", end='\\r')\n",
    "        courses.append(parse_course_block(cb, parser))\n",
    "    \n",
    "    print(f\"   âœ“ Completed all {total_courses} courses in {dept_code}     \")\n",
    "    return courses\n",
    "\n",
    "def scrape_all_courses(parser: RequisiteParser, only=None):\n",
    "    \"\"\"Scrape all courses using the LLM-based parser.\"\"\"\n",
    "    department_links = get_department_links(only=only)\n",
    "    all_courses = {}\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Starting scrape of {len(department_links)} departments\\n\")\n",
    "\n",
    "    for dept_idx, (dept_code, url) in enumerate(department_links, 1):\n",
    "        try:\n",
    "            print(f\"ğŸ“š [{dept_idx}/{len(department_links)}] Scraping {dept_code}...\")\n",
    "            dept_start_time = time.time()\n",
    "            \n",
    "            courses = parse_department(url, parser, dept_code)\n",
    "            all_courses[dept_code] = courses\n",
    "            \n",
    "            dept_elapsed = time.time() - dept_start_time\n",
    "            print(f\"âœ… Successfully scraped {dept_code} in {dept_elapsed/60:.1f} minutes\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error scraping {dept_code}: {e}\\n\")\n",
    "    \n",
    "    return all_courses\n",
    "\n",
    "def save_to_json(data, filename=\"unc_courses.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ğŸ’¾ Saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f48d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Starting scrape of 1 departments\n",
      "\n",
      "ğŸ“š [1/1] Scraping COMP...\n",
      "   Found 109 courses to parse\n",
      "   âœ“ Completed all 109 courses in COMP     \n",
      "âœ… Successfully scraped COMP in 3.0 minutes\n",
      "\n",
      "ğŸ’¾ Saved to unc_courses.json\n",
      "\n",
      "ğŸ“Š Statistics:\n",
      "â±ï¸  Total scraping time: 3.0 minutes\n",
      "   Total API calls: 79\n",
      "   Failed parses: 68\n",
      "\n",
      "âš ï¸  Failed to parse requisites for:\n",
      "   - COMP 401: 429 You exceeded your current quota, please check ...\n",
      "   - COMP 410: 429 You exceeded your current quota, please check ...\n",
      "   - COMP 411: 429 You exceeded your current quota, please check ...\n",
      "   - COMP 421: 429 You exceeded your current quota, please check ...\n",
      "   - COMP 423: 429 You exceeded your current quota, please check ...\n",
      "   ... and 63 more\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Main Execution\n",
    "parser = RequisiteParser(delay=2.1)\n",
    "\n",
    "# Overall timing\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# Scrape departments\n",
    "sample_departments = {\"COMP\"}\n",
    "courses = scrape_all_courses(parser, only=sample_departments)\n",
    "save_to_json(courses, \"output/unc_courses.json\")\n",
    "\n",
    "overall_elapsed = time.time() - overall_start_time\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nğŸ“Š Statistics:\")\n",
    "print(f\"   Total scraping time: {overall_elapsed/60:.1f} minutes\")\n",
    "print(f\"   Total API calls: {parser.api_calls}\")\n",
    "print(f\"   Failed parses: {len(parser.failed_parses)}\")\n",
    "\n",
    "if parser.failed_parses:\n",
    "    print(\"\\nâš ï¸  Failed to parse requisites for:\")\n",
    "    for course_id, error in parser.failed_parses[:5]:\n",
    "        print(f\"   - {course_id}: {error}\")\n",
    "    if len(parser.failed_parses) > 5:\n",
    "        print(f\"   ... and {len(parser.failed_parses) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e465a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing 152 departments...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning departments: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [01:04<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Requisite Analysis Complete!\n",
      "\n",
      "Total courses across all departments: 10212\n",
      "Courses with requisites: 2759\n",
      "Courses without requisites: 7453\n",
      "Percentage with requisites: 27.0%\n",
      "\n",
      "ğŸ’¡ You will need 2759 API calls\n",
      "â±ï¸  Estimated time at 2.1s/call: 96.6 minutes\n",
      "\n",
      "ğŸ“ˆ Top 10 departments by requisite count:\n",
      "   BIOL: 167/264 courses (63%)\n",
      "   PSYC: 101/183 courses (55%)\n",
      "   NURS: 91/169 courses (54%)\n",
      "   ECON: 81/145 courses (56%)\n",
      "   COMP: 79/109 courses (72%)\n",
      "   PHCY: 75/112 courses (67%)\n",
      "   CHEM: 74/112 courses (66%)\n",
      "   SPAN: 71/119 courses (60%)\n",
      "   MATH: 70/110 courses (64%)\n",
      "   COMM: 69/213 courses (32%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell to count non-empty requisites across all departments\n",
    "def count_requisites(only=None):\n",
    "    \"\"\"Count how many courses have non-empty requisites across departments.\"\"\"\n",
    "    department_links = get_department_links(only=only)\n",
    "    \n",
    "    total_courses = 0\n",
    "    courses_with_requisites = 0\n",
    "    dept_stats = {}\n",
    "    \n",
    "    print(f\"ğŸ” Analyzing {len(department_links)} departments...\\n\")\n",
    "    \n",
    "    for dept_code, url in tqdm(department_links, desc=\"Scanning departments\"):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        course_blocks = soup.find_all(\"div\", class_=\"courseblock\")\n",
    "        \n",
    "        dept_total = len(course_blocks)\n",
    "        dept_with_reqs = 0\n",
    "        \n",
    "        for block in course_blocks:\n",
    "            req_span = block.find(\"span\", class_=\"text detail-requisites margin--default\")\n",
    "            if req_span and req_span.text.strip() and req_span.text.strip() != \"Requisites:\":\n",
    "                dept_with_reqs += 1\n",
    "        \n",
    "        dept_stats[dept_code] = {\n",
    "            \"total\": dept_total,\n",
    "            \"with_requisites\": dept_with_reqs,\n",
    "            \"percentage\": (dept_with_reqs / dept_total * 100) if dept_total > 0 else 0\n",
    "        }\n",
    "        \n",
    "        total_courses += dept_total\n",
    "        courses_with_requisites += dept_with_reqs\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nğŸ“Š Requisite Analysis Complete!\\n\")\n",
    "    print(f\"Total courses across all departments: {total_courses}\")\n",
    "    print(f\"Courses with requisites: {courses_with_requisites}\")\n",
    "    print(f\"Courses without requisites: {total_courses - courses_with_requisites}\")\n",
    "    print(f\"Percentage with requisites: {courses_with_requisites/total_courses*100:.1f}%\")\n",
    "    print(f\"\\nğŸ’¡ You will need {courses_with_requisites} API calls\")\n",
    "    print(f\"â±ï¸  Estimated time at 2.1s/call: {courses_with_requisites * 2.1 / 60:.1f} minutes\")\n",
    "    \n",
    "    # Show top departments by requisite count\n",
    "    print(f\"\\nğŸ“ˆ Top 10 departments by requisite count:\")\n",
    "    sorted_depts = sorted(dept_stats.items(), key=lambda x: x[1]['with_requisites'], reverse=True)[:10]\n",
    "    for dept, stats in sorted_depts:\n",
    "        print(f\"   {dept}: {stats['with_requisites']}/{stats['total']} courses ({stats['percentage']:.0f}%)\")\n",
    "    \n",
    "    return dept_stats\n",
    "\n",
    "# Run the analysis\n",
    "# For all departments:\n",
    "dept_stats = count_requisites()\n",
    "\n",
    "# Or for specific departments:\n",
    "# dept_stats = count_requisites(only={\"COMP\", \"MATH\", \"BIOL\", \"CHEM\", \"PHYS\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
