{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9354f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malac\\anaconda3\\envs\\pathfinder\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Setup\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc02dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configure API & Constants\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "BASE_URL = \"https://catalog.unc.edu\"\n",
    "PROGRAMS_URL = f\"{BASE_URL}/undergraduate/programs-study/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d957173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: RequirementsParser Class\n",
    "class RequirementsParser:\n",
    "    def __init__(self, model=\"gemini-2.0-flash-lite\", delay: float = 0.5):\n",
    "        \"\"\"Initialize parser with Gemini API.\"\"\"\n",
    "        self.model = genai.GenerativeModel(model)\n",
    "        self.delay = delay\n",
    "        self.api_calls = 0\n",
    "        self.failed_parses = []\n",
    "        self.last_call_time = 0\n",
    "        \n",
    "    def parse_requirements(self, html_content: str, program_name: str = None) -> dict:\n",
    "        \"\"\"Parse requirements from HTML using Gemini API.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        requirements_div = soup.find('div', {'id': 'requirementstextcontainer'})\n",
    "        if not requirements_div:\n",
    "            requirements_div = soup.find('div', {'id': 'right-col'})\n",
    "        if not requirements_div:\n",
    "            return {\"error\": \"No requirements content found\", \"requirements\": {}}\n",
    "\n",
    "        requirements_html = str(requirements_div)\n",
    "        # Rate limiting\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_call_time\n",
    "        if elapsed < self.delay:\n",
    "            time.sleep(self.delay - elapsed)\n",
    "        self.last_call_time = time.time()\n",
    "        self.api_calls += 1\n",
    "\n",
    "        prompt = f\"\"\"Parse the following major/minor requirements HTML and return a JSON object with this structure:\n",
    "\n",
    "{{\n",
    "    \"program_type\": \"...\",\n",
    "    \"degree_type\": \"...\",\n",
    "    \"total_hours\": ...,\n",
    "    \"requirements\": {{ ... }},\n",
    "    \"footnotes\": [...],\n",
    "    \"special_notes\": \"...\"\n",
    "}}\n",
    "\n",
    "HTML content:\n",
    "{requirements_html}\n",
    "\n",
    "Return ONLY the JSON object.\"\"\"\n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            json_text = response.text.strip()\n",
    "            json_text = re.sub(r'^```json\\s*', '', json_text)\n",
    "            json_text = re.sub(r'\\s*```$', '', json_text)\n",
    "            return json.loads(json_text)\n",
    "        except Exception as e:\n",
    "            if program_name:\n",
    "                self.failed_parses.append((program_name, str(e)))\n",
    "            return {\n",
    "                \"error\": f\"Failed to parse: {str(e)}\",\n",
    "                \"requirements\": {},\n",
    "                \"program_name\": program_name\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f58b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Scraping Functions\n",
    "def get_program_links():\n",
    "    \"\"\"Get all program links from the programs page.\"\"\"\n",
    "    resp = requests.get(PROGRAMS_URL)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    sitemap = soup.find('div', {'class': 'az_sitemap'})\n",
    "    if not sitemap:\n",
    "        print(\"Warning: Could not find az_sitemap div\")\n",
    "        return []\n",
    "    links = []\n",
    "    for a in sitemap.find_all('a', href=True):\n",
    "        href, text = a['href'], a.text.strip()\n",
    "        if 'programs-study' not in href:\n",
    "            continue\n",
    "        full = urljoin(BASE_URL, href)\n",
    "        if '#' not in full:\n",
    "            full += '#requirementstext'\n",
    "        links.append({\n",
    "            'name': text,\n",
    "            'url': full,\n",
    "            'program_id': href.split('/')[-2]\n",
    "        })\n",
    "    return links\n",
    "\n",
    "def scrape_program(url, parser: RequirementsParser, info: dict):\n",
    "    \"\"\"Scrape a single program's requirements.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url); resp.raise_for_status()\n",
    "        result = parser.parse_requirements(resp.text, info['name'])\n",
    "        result.update({\n",
    "            'program_name': info['name'],\n",
    "            'program_id': info['program_id'],\n",
    "            'url': url\n",
    "        })\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {info['name']}: {e}\")\n",
    "        return {'program_name': info['name'], 'program_id': info['program_id'], 'url': url, 'error': str(e)}\n",
    "\n",
    "def scrape_all_programs(parser: RequirementsParser, limit=None):\n",
    "    \"\"\"Scrape all programs.\"\"\"\n",
    "    print(\"ðŸ” Finding all program links...\")\n",
    "    links = get_program_links()\n",
    "    if limit:\n",
    "        links = links[:limit]\n",
    "    print(f\"ðŸŽ¯ Found {len(links)} programs\\n\")\n",
    "    all_progs = []\n",
    "    start_all = time.time()\n",
    "    for i, info in enumerate(links, 1):\n",
    "        print(f\"ðŸ“š [{i}/{len(links)}] Scraping {info['name']}...\")\n",
    "        start = time.time()\n",
    "        res = scrape_program(info['url'], parser, info)\n",
    "        all_progs.append(res)\n",
    "        status = \"âœ…\" if 'error' not in res else \"âš ï¸\"\n",
    "        print(f\"{status} Completed in {time.time()-start:.1f}s\\n\")\n",
    "    print(f\"â±ï¸ Total time: {(time.time()-start_all)/60:.1f}m\")\n",
    "    return all_progs\n",
    "\n",
    "def save_to_json(data, filename=\"unc_programs.json\"):\n",
    "    \"\"\"Save data to JSON.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ðŸ’¾ Saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e7a016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Analysis Utilities\n",
    "def analyze_programs(programs):\n",
    "    \"\"\"Print summary of scraped programs.\"\"\"\n",
    "    total = len(programs)\n",
    "    success = len([p for p in programs if 'error' not in p])\n",
    "    failed = total - success\n",
    "    print(f\"\\nðŸ“Š Summary: total={total}, success={success}, failed={failed}\")\n",
    "    if failed:\n",
    "        print(\"\\nâš ï¸  Failed parses:\")\n",
    "        for p in programs:\n",
    "            if 'error' in p:\n",
    "                print(f\" - {p['program_name']}: {p['error'][:50]}...\")\n",
    "    majors = [p for p in programs if 'major' in p.get('program_name','').lower()]\n",
    "    minors = [p for p in programs if 'minor' in p.get('program_name','').lower()]\n",
    "    print(f\"\\nðŸ“ˆ Types: majors={len(majors)}, minors={len(minors)}, other={total-len(majors)-len(minors)}\")\n",
    "\n",
    "def find_program(programs, term):\n",
    "    \"\"\"Lookup programs by name.\"\"\"\n",
    "    term = term.lower()\n",
    "    matches = [p for p in programs if term in p['program_name'].lower()]\n",
    "    print(f\"\\nðŸ” Found {len(matches)} programs matching '{term}':\")\n",
    "    for p in matches:\n",
    "        print(f\"ðŸ“‹ {p['program_name']} - URL: {p['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d91161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing with first 5 programs...\n",
      "\n",
      "ðŸ” Finding all program links...\n",
      "ðŸŽ¯ Found 5 programs\n",
      "\n",
      "ðŸ“š [1/5] Scraping Aerospace Studies Minor...\n",
      "âœ… Completed in 2.7s\n",
      "\n",
      "ðŸ“š [2/5] Scraping African American and Diaspora Studies Minor...\n",
      "âœ… Completed in 3.9s\n",
      "\n",
      "ðŸ“š [3/5] Scraping African Studies Minor...\n",
      "âœ… Completed in 3.2s\n",
      "\n",
      "ðŸ“š [4/5] Scraping African, African American, and Diaspora Studies Major, B.A....\n",
      "âœ… Completed in 2.2s\n",
      "\n",
      "ðŸ“š [5/5] Scraping American Indian and Indigenous Studies Minor...\n",
      "âœ… Completed in 3.8s\n",
      "\n",
      "â±ï¸ Total time: 0.3m\n",
      "ðŸ’¾ Saved to unc_programs_test.json\n",
      "\n",
      "ðŸ“Š Summary: total=5, success=5, failed=0\n",
      "\n",
      "ðŸ“ˆ Types: majors=1, minors=4, other=0\n",
      "\n",
      "ðŸ“Š API calls: 5, Failed parses: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main Execution\n",
    "parser = RequirementsParser(model=\"gemini-2.0-flash-lite\", delay=2.1)\n",
    "\n",
    "# Test with first 5 programs\n",
    "print(\"ðŸ§ª Testing with first 5 programs...\\n\")\n",
    "test_progs = scrape_all_programs(parser, limit=5)\n",
    "save_to_json(test_progs, \"../output/unc_programs_test.json\")\n",
    "analyze_programs(test_progs)\n",
    "\n",
    "# Uncomment to scrape all programs\n",
    "# print(\"ðŸš€ Scraping all programs...\\n\")\n",
    "# all_progs = scrape_all_programs(parser)\n",
    "# save_to_json(all_progs, \"../output/unc_programs.json\")\n",
    "# analyze_programs(all_progs)\n",
    "\n",
    "# API usage stats\n",
    "print(f\"\\nðŸ“Š API calls: {parser.api_calls}, Failed parses: {len(parser.failed_parses)}\")\n",
    "if parser.failed_parses:\n",
    "    for name, err in parser.failed_parses[:5]:\n",
    "        print(f\" - {name}: {err[:50]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
