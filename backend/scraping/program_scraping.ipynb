{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9354f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malac\\anaconda3\\envs\\pathfinder\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import and setup\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor, Json\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc02dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configure API and Database\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "# Set GSSAPI disable for Windows\n",
    "os.environ['PGGSSENCMODE'] = 'disable'\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
    "\n",
    "if not DATABASE_URL:\n",
    "    raise ValueError(\"DATABASE_URL not found in .env file\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "BASE_URL = \"https://catalog.unc.edu\"\n",
    "PROGRAMS_URL = f\"{BASE_URL}/undergraduate/programs-study/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d957173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Database Manager for Programs\n",
    "class ProgramDatabaseManager:\n",
    "    def __init__(self, db_url: str):\n",
    "        \"\"\"Initialize database connection for programs.\"\"\"\n",
    "        # Parse the URL to add gssencmode parameter\n",
    "        from urllib.parse import urlparse\n",
    "        url = urlparse(db_url)\n",
    "        \n",
    "        conn_params = {\n",
    "            \"host\": url.hostname,\n",
    "            \"port\": url.port,\n",
    "            \"database\": url.path[1:],  # Remove leading '/'\n",
    "            \"user\": url.username,\n",
    "            \"password\": url.password,\n",
    "            \"sslmode\": \"require\",\n",
    "            \"gssencmode\": \"disable\"  # Fix for Windows GSSAPI error\n",
    "        }\n",
    "        \n",
    "        self.conn = psycopg2.connect(**conn_params)\n",
    "        self.conn.autocommit = False\n",
    "        self.cur = self.conn.cursor(cursor_factory=RealDictCursor)\n",
    "        \n",
    "        # Cache for course lookups\n",
    "        self.course_id_cache = {}\n",
    "        self._load_course_cache()\n",
    "    \n",
    "    def _load_course_cache(self):\n",
    "        \"\"\"Load existing courses into cache for linking requirements.\"\"\"\n",
    "        self.cur.execute(\"SELECT id, course_id FROM courses\")\n",
    "        for row in self.cur.fetchall():\n",
    "            self.course_id_cache[row['course_id']] = row['id']\n",
    "        logger.info(f\"Loaded {len(self.course_id_cache)} courses into cache\")\n",
    "    \n",
    "    def save_program(self, program_data: Dict) -> Optional[int]:\n",
    "        \"\"\"Save a program to the database.\"\"\"\n",
    "        try:\n",
    "            # Infer program type from name if not provided\n",
    "            program_type = program_data.get('program_type')\n",
    "            if not program_type:\n",
    "                name_lower = program_data.get('program_name', '').lower()\n",
    "                if 'minor' in name_lower:\n",
    "                    program_type = 'minor'\n",
    "                elif 'major' in name_lower:\n",
    "                    program_type = 'major'\n",
    "                elif 'certificate' in name_lower:\n",
    "                    program_type = 'certificate'\n",
    "                else:\n",
    "                    program_type = 'major'  # Default assumption\n",
    "            \n",
    "            # Log what we're saving\n",
    "            logger.info(f\"Saving {program_data.get('program_name')} as {program_type}\")\n",
    "            \n",
    "            # Insert or update program\n",
    "            self.cur.execute(\"\"\"\n",
    "                INSERT INTO programs \n",
    "                (program_id, name, program_type, degree_type, total_hours, url)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (program_id) DO UPDATE SET\n",
    "                    name = EXCLUDED.name,\n",
    "                    program_type = EXCLUDED.program_type,\n",
    "                    degree_type = EXCLUDED.degree_type,\n",
    "                    total_hours = EXCLUDED.total_hours,\n",
    "                    url = EXCLUDED.url,\n",
    "                    updated_at = NOW()\n",
    "                RETURNING id\n",
    "            \"\"\", (\n",
    "                program_data.get('program_id'),\n",
    "                program_data.get('program_name'),\n",
    "                program_type,  # Use inferred type\n",
    "                program_data.get('degree_type'),\n",
    "                program_data.get('total_hours'),\n",
    "                program_data.get('url')\n",
    "            ))\n",
    "            \n",
    "            program_db_id = self.cur.fetchone()['id']\n",
    "            \n",
    "            # Save requirements if present\n",
    "            if program_data.get('requirements'):\n",
    "                self._save_program_requirements(program_db_id, program_data['requirements'])\n",
    "            \n",
    "            return program_db_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving program {program_data.get('program_name')}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _save_program_requirements(self, program_db_id: int, requirements: Dict):\n",
    "        \"\"\"Save requirements for a program.\"\"\"\n",
    "        # Clear existing requirements\n",
    "        self.cur.execute(\"\"\"\n",
    "            DELETE FROM program_requirement_courses \n",
    "            WHERE requirement_id IN (\n",
    "                SELECT id FROM program_requirements WHERE program_id = %s\n",
    "            )\n",
    "        \"\"\", (program_db_id,))\n",
    "        self.cur.execute(\"DELETE FROM program_requirements WHERE program_id = %s\", (program_db_id,))\n",
    "        \n",
    "        display_order = 0\n",
    "        \n",
    "        # Process different requirement types\n",
    "        req_type_mappings = [\n",
    "            ('gateway_courses', 'gateway'),\n",
    "            ('core_requirements', 'core'),\n",
    "            ('electives', 'elective'),\n",
    "            ('allied_sciences', 'allied_science')\n",
    "        ]\n",
    "        \n",
    "        for json_key, req_type in req_type_mappings:\n",
    "            if json_key not in requirements:\n",
    "                continue\n",
    "            \n",
    "            items = requirements[json_key]\n",
    "            if not isinstance(items, list):\n",
    "                continue\n",
    "            \n",
    "            for item in items:\n",
    "                # Determine category name and other fields based on structure\n",
    "                category_name = None\n",
    "                min_credits = None\n",
    "                min_courses = None\n",
    "                selection_notes = None\n",
    "                level_requirement = None\n",
    "                other_restrictions = None\n",
    "                \n",
    "                # Handle different item structures\n",
    "                if isinstance(item, dict):\n",
    "                    category_name = item.get('category', item.get('description', json_key))\n",
    "                    min_credits = item.get('min_credits', item.get('total_credits', item.get('credits')))\n",
    "                    min_courses = item.get('min_courses', item.get('count'))\n",
    "                    selection_notes = item.get('selection_notes', item.get('notes'))\n",
    "                    level_requirement = item.get('level_requirement')\n",
    "                    other_restrictions = item.get('restrictions', item.get('other_restrictions'))\n",
    "                \n",
    "                # Insert requirement category\n",
    "                self.cur.execute(\"\"\"\n",
    "                    INSERT INTO program_requirements\n",
    "                    (program_id, requirement_type, category_name, min_credits, \n",
    "                     min_courses, selection_notes, level_requirement, \n",
    "                     other_restrictions, display_order)\n",
    "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                    RETURNING id\n",
    "                \"\"\", (\n",
    "                    program_db_id,\n",
    "                    req_type,\n",
    "                    category_name,\n",
    "                    min_credits,\n",
    "                    min_courses,\n",
    "                    selection_notes,\n",
    "                    level_requirement,\n",
    "                    other_restrictions,\n",
    "                    display_order\n",
    "                ))\n",
    "                \n",
    "                req_id = self.cur.fetchone()['id']\n",
    "                display_order += 1\n",
    "                \n",
    "                # Extract and save course associations\n",
    "                self._save_requirement_courses(req_id, item)\n",
    "    \n",
    "    def _save_requirement_courses(self, req_id: int, item: Dict):\n",
    "        \"\"\"Save course associations for a requirement.\"\"\"\n",
    "        courses_to_add = []\n",
    "        \n",
    "        # Handle different structures for course lists\n",
    "        if 'courses' in item and isinstance(item['courses'], list):\n",
    "            for course in item['courses']:\n",
    "                if isinstance(course, dict):\n",
    "                    courses_to_add.append({\n",
    "                        'course_code': course.get('course_code', course.get('course')),\n",
    "                        'is_required': True\n",
    "                    })\n",
    "                elif isinstance(course, str):\n",
    "                    courses_to_add.append({'course_code': course, 'is_required': True})\n",
    "        \n",
    "        # Single course entries\n",
    "        elif 'course_code' in item:\n",
    "            courses_to_add.append({\n",
    "                'course_code': item['course_code'],\n",
    "                'is_required': True\n",
    "            })\n",
    "        \n",
    "        # Required courses list\n",
    "        elif 'required_courses' in item and isinstance(item['required_courses'], list):\n",
    "            for course_code in item['required_courses']:\n",
    "                courses_to_add.append({'course_code': course_code, 'is_required': True})\n",
    "        \n",
    "        # Optional courses lists\n",
    "        elif 'options' in item and isinstance(item['options'], list):\n",
    "            for course_code in item['options']:\n",
    "                courses_to_add.append({'course_code': course_code, 'is_required': False})\n",
    "        elif 'specific_options' in item and isinstance(item['specific_options'], list):\n",
    "            for course_code in item['specific_options']:\n",
    "                courses_to_add.append({'course_code': course_code, 'is_required': False})\n",
    "        \n",
    "        # Insert course associations\n",
    "        for course_info in courses_to_add:\n",
    "            course_code = course_info['course_code']\n",
    "            if not course_code:\n",
    "                continue\n",
    "                \n",
    "            course_db_id = self.course_id_cache.get(course_code)\n",
    "            if course_db_id:\n",
    "                try:\n",
    "                    self.cur.execute(\"\"\"\n",
    "                        INSERT INTO program_requirement_courses\n",
    "                        (requirement_id, course_id, is_required)\n",
    "                        VALUES (%s, %s, %s)\n",
    "                        ON CONFLICT (requirement_id, course_id) DO NOTHING\n",
    "                    \"\"\", (req_id, course_db_id, course_info['is_required']))\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to link course {course_code} to requirement: {e}\")\n",
    "    \n",
    "    def commit(self):\n",
    "        \"\"\"Commit the current transaction.\"\"\"\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def rollback(self):\n",
    "        \"\"\"Rollback the current transaction.\"\"\"\n",
    "        self.conn.rollback()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        self.cur.close()\n",
    "        self.conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1f58b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: RequirementsParser class\n",
    "class RequirementsParser:\n",
    "    def __init__(self, model=\"gemini-1.5-flash\", delay: float = 2.1):\n",
    "        \"\"\"Initialize the parser with Gemini API.\"\"\"\n",
    "        self.model = genai.GenerativeModel(model)\n",
    "        self.delay = delay\n",
    "        self.api_calls = 0\n",
    "        self.failed_parses = []\n",
    "        self.last_call_time = 0\n",
    "        \n",
    "    def parse_requirements(self, html_content: str, program_name: str = None) -> dict:\n",
    "        \"\"\"Parse requirements from HTML content using Gemini API.\"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Extract the right-col div content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Try to find the requirements tab content\n",
    "        requirements_div = soup.find('div', {'id': 'requirementstextcontainer'})\n",
    "        if not requirements_div:\n",
    "            # Fallback to right-col\n",
    "            requirements_div = soup.find('div', {'id': 'right-col'})\n",
    "        \n",
    "        if not requirements_div:\n",
    "            return {\n",
    "                \"error\": \"No requirements content found\",\n",
    "                \"requirements\": {}\n",
    "            }\n",
    "        \n",
    "        # Convert tables to structured text for better parsing\n",
    "        requirements_html = str(requirements_div)\n",
    "        \n",
    "        # Log the size of content being sent\n",
    "        content_size = len(requirements_html)\n",
    "        print(f\"   HTML content size: {content_size} characters\")\n",
    "        \n",
    "        # If content is too large, try to extract just the essential parts\n",
    "        if content_size > 50000:  # If over 50k characters\n",
    "            print(\"   Content too large, extracting essential parts...\")\n",
    "            # Extract just tables and important text\n",
    "            tables = requirements_div.find_all('table')\n",
    "            essential_html = \"\"\n",
    "            for table in tables:\n",
    "                essential_html += str(table)\n",
    "            requirements_html = essential_html\n",
    "            print(f\"   Reduced to: {len(requirements_html)} characters\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        current_time = time.time()\n",
    "        time_since_last_call = current_time - self.last_call_time\n",
    "        if time_since_last_call < self.delay:\n",
    "            sleep_time = self.delay - time_since_last_call\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_call_time = time.time()\n",
    "        self.api_calls += 1\n",
    "        \n",
    "        prompt = f\"\"\"Parse the following major/minor requirements HTML and return a JSON object with this structure:\n",
    "\n",
    "{{\n",
    "    \"program_type\": \"major\" or \"minor\" or \"certificate\" (REQUIRED - infer from the program name if not explicitly stated),\n",
    "    \"degree_type\": \"BA\" or \"BS\" or \"BSPH\" etc if specified,\n",
    "    \"total_hours\": // Total credit hours required (number or null),\n",
    "    \"requirements\": {{\n",
    "        \"gateway_courses\": [\n",
    "            {{\n",
    "                \"course_code\": \"BIOL 101\",\n",
    "                \"course_name\": \"Principles of Biology\",\n",
    "                \"credits\": 4,\n",
    "                \"lab_code\": \"BIOL 101L\", // if has a lab\n",
    "                \"notes\": \"any special notes\"\n",
    "            }}\n",
    "        ],\n",
    "        \"core_requirements\": [\n",
    "            {{\n",
    "                \"category\": \"Fundamentals Core Courses\", // or whatever the category is called\n",
    "                \"courses\": [\n",
    "                    {{\n",
    "                        \"course_code\": \"BIOL 103\",\n",
    "                        \"course_name\": \"How Cells Function\",\n",
    "                        \"credits\": 3,\n",
    "                        \"notes\": null\n",
    "                    }}\n",
    "                ],\n",
    "                \"total_credits\": 9,\n",
    "                \"selection_notes\": null // e.g., \"choose 2 from list\"\n",
    "            }}\n",
    "        ],\n",
    "        \"electives\": [\n",
    "            {{\n",
    "                \"category\": \"Biology electives\",\n",
    "                \"min_credits\": 12,\n",
    "                \"min_courses\": null,\n",
    "                \"level_requirement\": \"200 or above\",\n",
    "                \"specific_options\": [\"BIOL 220\", \"BIOL 240\"], // if specific courses listed\n",
    "                \"restrictions\": \"At least one course must have a laboratory\",\n",
    "                \"notes\": null\n",
    "            }}\n",
    "        ],\n",
    "        \"allied_sciences\": [\n",
    "            {{\n",
    "                \"category\": \"Chemistry\",\n",
    "                \"required_courses\": [\"CHEM 101\", \"CHEM 102\"],\n",
    "                \"min_credits\": 8,\n",
    "                \"notes\": \"With labs\"\n",
    "            }}\n",
    "        ],\n",
    "        \"additional_requirements\": [\n",
    "            \"Students must fulfill all General Education requirements\",\n",
    "            \"Minimum 2.0 GPA in major courses\"\n",
    "        ]\n",
    "    }},\n",
    "    \"footnotes\": [\n",
    "        // Any superscript notes from the tables\n",
    "    ],\n",
    "    \"special_notes\": // Any important notes about the program\n",
    "}}\n",
    "\n",
    "IMPORTANT: The program_type field is REQUIRED. Look at the program name being parsed:\n",
    "Program name: {program_name}\n",
    "- If it contains \"Minor\", set program_type to \"minor\"  \n",
    "- If it contains \"Major\", set program_type to \"major\"\n",
    "- If it contains \"Certificate\", set program_type to \"certificate\"\n",
    "- Default to \"major\" if unclear\n",
    "\n",
    "Parse the HTML carefully:\n",
    "1. Look for table structures with course codes, names, and credit hours\n",
    "2. Identify section headers (Gateway Course, Core Requirements, etc.)\n",
    "3. Extract exact course codes (DEPT ###) and credit hours\n",
    "4. Note any footnotes marked with superscripts\n",
    "5. Capture selection rules (e.g., \"Two of the following five Core Course options\")\n",
    "6. Include lab courses that are mentioned\n",
    "7. Extract total hour requirements if stated\n",
    "8. Note any GPA or grade requirements\n",
    "\n",
    "HTML content to parse:\n",
    "{requirements_html}\n",
    "\n",
    "Return ONLY the JSON object, no explanation or markdown.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            json_text = response.text.strip()\n",
    "            json_text = re.sub(r'^```json\\s*', '', json_text)\n",
    "            json_text = re.sub(r'\\s*```$', '', json_text)\n",
    "            \n",
    "            result = json.loads(json_text)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            if program_name:\n",
    "                self.failed_parses.append((program_name, str(e)))\n",
    "            return {\n",
    "                \"error\": f\"Failed to parse: {str(e)}\",\n",
    "                \"requirements\": {},\n",
    "                \"program_name\": program_name\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7a016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Scraping functions\n",
    "def get_program_links():\n",
    "    \"\"\"Get all program links from the programs page.\"\"\"\n",
    "    response = requests.get(PROGRAMS_URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the az_sitemap div\n",
    "    sitemap = soup.find('div', {'class': 'az_sitemap'})\n",
    "    if not sitemap:\n",
    "        print(\"Warning: Could not find az_sitemap div\")\n",
    "        return []\n",
    "    \n",
    "    links = []\n",
    "    for a in sitemap.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        text = a.text.strip()\n",
    "        \n",
    "        # Skip non-program links\n",
    "        if 'programs-study' not in href:\n",
    "            continue\n",
    "            \n",
    "        # Get the full URL\n",
    "        full_url = urljoin(BASE_URL, href)\n",
    "        \n",
    "        # Add #requirementstext to the URL to go directly to requirements\n",
    "        if '#' not in full_url:\n",
    "            full_url += '#requirementstext'\n",
    "        \n",
    "        links.append({\n",
    "            'name': text,\n",
    "            'url': full_url,\n",
    "            'program_id': href.split('/')[-2] if '/' in href else href\n",
    "        })\n",
    "    \n",
    "    return links\n",
    "\n",
    "def scrape_program(url: str, parser: RequirementsParser, program_info: dict, \n",
    "                  db_manager: Optional[ProgramDatabaseManager] = None, mode: str = 'database'):\n",
    "    \"\"\"Scrape a single program's requirements.\"\"\"\n",
    "    try:\n",
    "        # Time the request\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        response = requests.get(url, timeout=30)  # Add 30 second timeout\n",
    "        response.raise_for_status()\n",
    "        request_time = time.time() - start_time\n",
    "        print(f\"   Request took: {request_time:.1f}s\")\n",
    "        \n",
    "        # Time the parsing\n",
    "        parse_start = time.time()\n",
    "        result = parser.parse_requirements(response.text, program_info['name'])\n",
    "        parse_time = time.time() - parse_start\n",
    "        print(f\"   Parsing took: {parse_time:.1f}s\")\n",
    "        \n",
    "        # Add program metadata\n",
    "        result['program_name'] = program_info['name']\n",
    "        result['program_id'] = program_info['program_id']\n",
    "        result['url'] = url\n",
    "        \n",
    "        # Save to database if in database mode\n",
    "        if mode in ['database', 'both'] and db_manager and 'error' not in result:\n",
    "            try:\n",
    "                save_start = time.time()\n",
    "                db_manager.save_program(result)\n",
    "                save_time = time.time() - save_start\n",
    "                print(f\"   Database save took: {save_time:.1f}s\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to save program {program_info['name']}: {e}\")\n",
    "                result['database_error'] = str(e)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping {program_info['name']}: {e}\")\n",
    "        return {\n",
    "            'program_name': program_info['name'],\n",
    "            'program_id': program_info['program_id'],\n",
    "            'url': url,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def scrape_all_programs(parser: RequirementsParser, db_manager: Optional[ProgramDatabaseManager] = None,\n",
    "                       limit: Optional[int] = None, mode: str = 'database', dry_run: bool = False):\n",
    "    \"\"\"\n",
    "    Scrape all programs with flexible output options.\n",
    "    \n",
    "    Args:\n",
    "        parser: RequirementsParser instance\n",
    "        db_manager: ProgramDatabaseManager instance (required for database mode)\n",
    "        limit: Maximum number of programs to scrape (None for all)\n",
    "        mode: 'database', 'json', or 'both'\n",
    "        dry_run: If True, don't actually save anything\n",
    "    \"\"\"\n",
    "    # Get all program links\n",
    "    print(\"üîç Finding all program links...\")\n",
    "    program_links = get_program_links()\n",
    "    \n",
    "    if limit:\n",
    "        program_links = program_links[:limit]\n",
    "    \n",
    "    print(f\"\\nüéØ Found {len(program_links)} programs to scrape\")\n",
    "    print(f\"   Mode: {mode}\")\n",
    "    print(f\"   Dry run: {dry_run}\\n\")\n",
    "    \n",
    "    all_programs = []\n",
    "    overall_start_time = time.time()\n",
    "    saved_count = 0\n",
    "    \n",
    "    for idx, program_info in enumerate(program_links, 1):\n",
    "        print(f\"üìö [{idx}/{len(program_links)}] Scraping {program_info['name']}...\")\n",
    "        program_start_time = time.time()\n",
    "        \n",
    "        # Begin transaction for this program\n",
    "        if db_manager and not dry_run:\n",
    "            db_manager.conn.commit()  # Commit any pending changes\n",
    "        \n",
    "        result = scrape_program(\n",
    "            program_info['url'], \n",
    "            parser, \n",
    "            program_info,\n",
    "            db_manager if not dry_run else None,\n",
    "            mode\n",
    "        )\n",
    "        \n",
    "        if mode in ['json', 'both']:\n",
    "            all_programs.append(result)\n",
    "        \n",
    "        # Commit program transaction\n",
    "        if db_manager and not dry_run and mode in ['database', 'both'] and 'error' not in result:\n",
    "            try:\n",
    "                db_manager.commit()\n",
    "                saved_count += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to commit {program_info['name']}: {e}\")\n",
    "                db_manager.rollback()\n",
    "        \n",
    "        program_elapsed = time.time() - program_start_time\n",
    "        status = \"‚úÖ\" if 'error' not in result else \"‚ö†Ô∏è\"\n",
    "        print(f\"{status} Completed {program_info['name']} in {program_elapsed:.1f} seconds\\n\")\n",
    "    \n",
    "    overall_elapsed = time.time() - overall_start_time\n",
    "    print(f\"‚è±Ô∏è  Total scraping time: {overall_elapsed/60:.1f} minutes\")\n",
    "    print(f\"üíæ Saved {saved_count} programs to database\")\n",
    "    \n",
    "    return all_programs\n",
    "\n",
    "def save_to_json(data, filename=\"unc_programs.json\"):\n",
    "    \"\"\"Save program data to JSON file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nüíæ Saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83d91161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Analysis functions\n",
    "def analyze_programs(programs):\n",
    "    \"\"\"Analyze the scraped program data.\"\"\"\n",
    "    total = len(programs)\n",
    "    successful = len([p for p in programs if 'error' not in p])\n",
    "    failed = total - successful\n",
    "    \n",
    "    print(f\"\\nüìä Scraping Summary:\")\n",
    "    print(f\"   Total programs: {total}\")\n",
    "    print(f\"   Successfully parsed: {successful}\")\n",
    "    print(f\"   Failed to parse: {failed}\")\n",
    "    \n",
    "    if failed > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Failed programs:\")\n",
    "        for p in programs:\n",
    "            if 'error' in p:\n",
    "                print(f\"   - {p['program_name']}: {p['error'][:50]}...\")\n",
    "    \n",
    "    # Count majors vs minors\n",
    "    majors = [p for p in programs if 'major' in p.get('program_name', '').lower()]\n",
    "    minors = [p for p in programs if 'minor' in p.get('program_name', '').lower()]\n",
    "    \n",
    "    print(f\"\\nüìà Program Types:\")\n",
    "    print(f\"   Majors: {len(majors)}\")\n",
    "    print(f\"   Minors: {len(minors)}\")\n",
    "    print(f\"   Other: {total - len(majors) - len(minors)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b12fa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 12:21:01,881 - INFO - Loaded 123 courses into cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing with first 5 programs...\n",
      "\n",
      "üîç Finding all program links...\n",
      "\n",
      "üéØ Found 5 programs to scrape\n",
      "   Mode: database\n",
      "   Dry run: False\n",
      "\n",
      "üìö [1/5] Scraping Aerospace Studies Minor...\n",
      "   Request took: 0.2s\n",
      "   HTML content size: 4792 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 12:21:54,986 - INFO - Saving Aerospace Studies Minor as minor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Parsing took: 52.6s\n",
      "   Database save took: 0.6s\n",
      "‚úÖ Completed Aerospace Studies Minor in 53.4 seconds\n",
      "\n",
      "üìö [2/5] Scraping African American and Diaspora Studies Minor...\n",
      "   Request took: 0.3s\n",
      "   HTML content size: 7509 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 12:22:13,046 - INFO - Saving African American and Diaspora Studies Minor as minor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Parsing took: 17.1s\n",
      "   Database save took: 0.8s\n",
      "‚úÖ Completed African American and Diaspora Studies Minor in 18.4 seconds\n",
      "\n",
      "üìö [3/5] Scraping African Studies Minor...\n",
      "   Request took: 0.3s\n",
      "   HTML content size: 6237 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 12:22:21,610 - INFO - Saving African Studies Minor as minor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Parsing took: 7.3s\n",
      "   Database save took: 0.4s\n",
      "‚úÖ Completed African Studies Minor in 8.0 seconds\n",
      "\n",
      "üìö [4/5] Scraping African, African American, and Diaspora Studies Major, B.A....\n",
      "   Request took: 0.2s\n",
      "   HTML content size: 13667 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 12:22:30,908 - INFO - Saving African, African American, and Diaspora Studies Major, B.A. as major\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Parsing took: 8.6s\n",
      "   Database save took: 1.4s\n",
      "‚úÖ Completed African, African American, and Diaspora Studies Major, B.A. in 10.4 seconds\n",
      "\n",
      "üìö [5/5] Scraping American Indian and Indigenous Studies Minor...\n",
      "   Request took: 0.3s\n",
      "   HTML content size: 19072 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 12:26:53,159 - INFO - Saving American Indian and Indigenous Studies Minor as minor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Parsing took: 260.4s\n",
      "   Database save took: 0.2s\n",
      "‚úÖ Completed American Indian and Indigenous Studies Minor in 261.0 seconds\n",
      "\n",
      "‚è±Ô∏è  Total scraping time: 5.9 minutes\n",
      "üíæ Saved 5 programs to database\n",
      "\n",
      "üìä API Statistics:\n",
      "   Total API calls: 5\n",
      "   Failed parses: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Main execution\n",
    "# Initialize components\n",
    "parser = RequirementsParser(model=\"gemini-1.5-flash\", delay=0.5)  # Reduced delay\n",
    "db_manager = ProgramDatabaseManager(DATABASE_URL)\n",
    "\n",
    "# Configuration options\n",
    "MODE = 'database'  # 'database', 'json', or 'both'\n",
    "DRY_RUN = False    # Set to True to test without saving\n",
    "LIMIT = 5          # Set to None to scrape all programs\n",
    "\n",
    "# Test with a few programs first\n",
    "print(\"üß™ Testing with first 5 programs...\\n\")\n",
    "test_programs = scrape_all_programs(\n",
    "    parser,\n",
    "    db_manager,\n",
    "    limit=LIMIT,\n",
    "    mode=MODE,\n",
    "    dry_run=DRY_RUN\n",
    ")\n",
    "\n",
    "# Save JSON backup if requested\n",
    "if MODE in ['json', 'both'] and test_programs:\n",
    "    save_to_json(test_programs, \"unc_programs_test.json\")\n",
    "    analyze_programs(test_programs)\n",
    "\n",
    "# Option to scrape all programs (uncomment to use)\n",
    "# print(\"\\nüöÄ Scraping all programs...\\n\")\n",
    "# all_programs = scrape_all_programs(parser, db_manager, mode=MODE)\n",
    "# if MODE in ['json', 'both'] and all_programs:\n",
    "#     save_to_json(all_programs, \"unc_programs_all.json\")\n",
    "#     analyze_programs(all_programs)\n",
    "\n",
    "# Print API usage stats\n",
    "print(f\"\\nüìä API Statistics:\")\n",
    "print(f\"   Total API calls: {parser.api_calls}\")\n",
    "print(f\"   Failed parses: {len(parser.failed_parses)}\")\n",
    "if parser.failed_parses:\n",
    "    print(f\"\\n   Failed programs:\")\n",
    "    for name, error in parser.failed_parses[:5]:\n",
    "        print(f\"   - {name}: {error[:50]}...\")\n",
    "\n",
    "# Close database connection\n",
    "db_manager.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8176ec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m                         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m         Courses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(req[\u001b[33m'\u001b[39m\u001b[33mcourses\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m linked\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Run verification\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mverify_program_scraping\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mverify_program_scraping\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Verify what was scraped into the database.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb_queries\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CourseDatabase\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mCourseDatabase\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m db:\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Get program stats\u001b[39;00m\n\u001b[32m      8\u001b[39m     stats = db.get_database_stats()\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîç Program Statistics:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\malac\\pathfinder\\scraping\\db_queries.py:27\u001b[39m, in \u001b[36mCourseDatabase.__init__\u001b[39m\u001b[34m(self, db_url)\u001b[39m\n\u001b[32m     15\u001b[39m url = urlparse(db_url)\n\u001b[32m     17\u001b[39m conn_params = {\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m\"\u001b[39m: url.hostname,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m: url.port,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgssencmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdisable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28mself\u001b[39m.conn = \u001b[43mpsycopg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconn_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mself\u001b[39m.cur = \u001b[38;5;28mself\u001b[39m.conn.cursor(cursor_factory=RealDictCursor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\malac\\anaconda3\\envs\\pathfinder\\Lib\\site-packages\\psycopg2\\__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Database verification\n",
    "def verify_program_scraping():\n",
    "    \"\"\"Verify what was scraped into the database.\"\"\"\n",
    "    from db_queries import CourseDatabase\n",
    "    \n",
    "    with CourseDatabase() as db:\n",
    "        # Get program stats\n",
    "        stats = db.get_database_stats()\n",
    "        print(f\"\\nüîç Program Statistics:\")\n",
    "        print(f\"   Total programs: {stats['total']}\")\n",
    "        print(f\"   Majors: {stats['majors']}\")\n",
    "        print(f\"   Minors: {stats['minors']}\")\n",
    "        \n",
    "        # Test program lookup\n",
    "        programs = db.search_programs(\"computer science\")\n",
    "        if programs:\n",
    "            print(f\"\\nüìö Found {len(programs)} Computer Science programs:\")\n",
    "            for prog in programs:\n",
    "                print(f\"   - {prog['name']} ({prog['program_type']})\")\n",
    "                \n",
    "                # Get requirements\n",
    "                reqs = db.get_program_requirements(prog['program_id'])\n",
    "                print(f\"     Requirements: {len(reqs)} categories\")\n",
    "                for req in reqs[:2]:  # Show first 2\n",
    "                    print(f\"       ‚Ä¢ {req['category_name']} ({req['requirement_type']})\")\n",
    "                    if req['courses']:\n",
    "                        print(f\"         Courses: {len(req['courses'])} linked\")\n",
    "\n",
    "# Run verification\n",
    "verify_program_scraping()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
