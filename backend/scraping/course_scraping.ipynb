{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c3a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malac\\anaconda3\\envs\\pathfinder\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Setup\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor, Json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344947c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configure API & Database\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "DATABASE_URL   = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
    "if not DATABASE_URL:\n",
    "    raise ValueError(\"DATABASE_URL not found in .env file\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "BASE_URL        = \"https://catalog.unc.edu\"\n",
    "COURSE_INDEX_URL = f\"{BASE_URL}/courses/#text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c1151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Database Manager class\n",
    "class DatabaseManager:\n",
    "    def __init__(self, db_url: str):\n",
    "        \"\"\"Initialize database connection and caches.\"\"\"\n",
    "        # Parse the URL\n",
    "        url = urlparse(db_url)\n",
    "        \n",
    "        conn_params = {\n",
    "            \"host\": url.hostname,\n",
    "            \"port\": url.port,\n",
    "            \"database\": url.path[1:],\n",
    "            \"user\": url.username,\n",
    "            \"password\": url.password,\n",
    "            \"sslmode\": \"require\",\n",
    "            \"gssencmode\": \"disable\"\n",
    "        }\n",
    "        \n",
    "        self.conn = psycopg2.connect(**conn_params)\n",
    "        self.conn.autocommit = False\n",
    "        self.cur = self.conn.cursor(cursor_factory=RealDictCursor)\n",
    "        \n",
    "        # Cache for lookups\n",
    "        self.department_cache = {}\n",
    "        self.course_id_cache = {}\n",
    "        \n",
    "        # Load existing data into cache\n",
    "        self._load_cache()\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        \"\"\"Load existing departments and courses into cache.\"\"\"\n",
    "        # Load departments\n",
    "        self.cur.execute(\"SELECT id, code FROM departments\")\n",
    "        for row in self.cur.fetchall():\n",
    "            self.department_cache[row['code']] = row['id']\n",
    "        \n",
    "        # Load course IDs\n",
    "        self.cur.execute(\"SELECT id, course_id FROM courses\")\n",
    "        for row in self.cur.fetchall():\n",
    "            self.course_id_cache[row['course_id']] = row['id']\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.department_cache)} departments and {len(self.course_id_cache)} courses into cache\")\n",
    "    \n",
    "    def get_or_create_department(self, dept_code: str) -> int:\n",
    "        \"\"\"Get or create a department, returning its ID.\"\"\"\n",
    "        if dept_code in self.department_cache:\n",
    "            return self.department_cache[dept_code]\n",
    "        \n",
    "        self.cur.execute(\"\"\"\n",
    "            INSERT INTO departments (code) \n",
    "            VALUES (%s) \n",
    "            ON CONFLICT (code) DO UPDATE SET code = EXCLUDED.code\n",
    "            RETURNING id\n",
    "        \"\"\", (dept_code,))\n",
    "        \n",
    "        dept_id = self.cur.fetchone()['id']\n",
    "        self.department_cache[dept_code] = dept_id\n",
    "        return dept_id\n",
    "    \n",
    "    def save_course(self, course_data: Dict) -> Optional[int]:\n",
    "        \"\"\"Save a course to the database.\"\"\"\n",
    "        try:\n",
    "            dept_id = self.get_or_create_department(course_data['department'])\n",
    "            \n",
    "            # Extract gen_ed as array\n",
    "            gen_ed = []\n",
    "            if course_data.get('gen_ed'):\n",
    "                gen_ed = [course_data['gen_ed']] if isinstance(course_data['gen_ed'], str) else course_data['gen_ed']\n",
    "            \n",
    "            # Insert or update course\n",
    "            self.cur.execute(\"\"\"\n",
    "                INSERT INTO courses \n",
    "                (course_id, department_id, course_number, name, description, \n",
    "                 credits, gen_ed, grading_status)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (course_id) DO UPDATE SET\n",
    "                    name = EXCLUDED.name,\n",
    "                    description = EXCLUDED.description,\n",
    "                    credits = EXCLUDED.credits,\n",
    "                    gen_ed = EXCLUDED.gen_ed,\n",
    "                    grading_status = EXCLUDED.grading_status,\n",
    "                    updated_at = NOW()\n",
    "                RETURNING id\n",
    "            \"\"\", (\n",
    "                course_data['course_id'],\n",
    "                dept_id,\n",
    "                course_data['course_number'],\n",
    "                course_data['course_name'],\n",
    "                course_data.get('description'),\n",
    "                course_data.get('credits'),\n",
    "                gen_ed,\n",
    "                course_data.get('grading_status')\n",
    "            ))\n",
    "            \n",
    "            course_db_id = self.cur.fetchone()['id']\n",
    "            self.course_id_cache[course_data['course_id']] = course_db_id\n",
    "            \n",
    "            # Save prerequisites if present\n",
    "            if course_data.get('requisites'):\n",
    "                self._save_prerequisites(course_db_id, course_data['requisites'])\n",
    "            \n",
    "            # Save grade requirements if present\n",
    "            if course_data.get('grade_requirements'):\n",
    "                self._save_grade_requirements(course_db_id, course_data['grade_requirements'])\n",
    "            \n",
    "            return course_db_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving course {course_data.get('course_id')}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _save_prerequisites(self, course_db_id: int, requisites: Dict):\n",
    "        \"\"\"Save prerequisites for a course.\"\"\"\n",
    "        # Clear existing prerequisites\n",
    "        self.cur.execute(\"DELETE FROM prerequisites WHERE course_id = %s\", (course_db_id,))\n",
    "        \n",
    "        # Save prerequisites (AND groups)\n",
    "        for group_idx, prereq_group in enumerate(requisites.get('prerequisites', [])):\n",
    "            for prereq_course_code in prereq_group:\n",
    "                prereq_db_id = self.course_id_cache.get(prereq_course_code.strip())\n",
    "                if prereq_db_id:\n",
    "                    self.cur.execute(\"\"\"\n",
    "                        INSERT INTO prerequisites \n",
    "                        (course_id, prereq_group, prereq_course_id, is_corequisite)\n",
    "                        VALUES (%s, %s, %s, %s)\n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", (course_db_id, group_idx, prereq_db_id, False))\n",
    "        \n",
    "        # Save corequisites\n",
    "        for group_idx, coreq_group in enumerate(requisites.get('corequisites', [])):\n",
    "            for coreq_course_code in coreq_group:\n",
    "                coreq_db_id = self.course_id_cache.get(coreq_course_code.strip())\n",
    "                if coreq_db_id:\n",
    "                    self.cur.execute(\"\"\"\n",
    "                        INSERT INTO prerequisites \n",
    "                        (course_id, prereq_group, prereq_course_id, is_corequisite)\n",
    "                        VALUES (%s, %s, %s, %s)\n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", (course_db_id, group_idx + 1000, coreq_db_id, True))\n",
    "    \n",
    "    def _save_grade_requirements(self, course_db_id: int, grade_requirements: Dict):\n",
    "        \"\"\"Save grade requirements for a course.\"\"\"\n",
    "        # Clear existing grade requirements\n",
    "        self.cur.execute(\"DELETE FROM grade_requirements WHERE course_id = %s\", (course_db_id,))\n",
    "        \n",
    "        for req_course_code, min_grade in grade_requirements.items():\n",
    "            # Try different formats\n",
    "            req_course_code = req_course_code.replace(' ', '')\n",
    "            req_db_id = None\n",
    "            for possible_code in [req_course_code, f\"{req_course_code[:4]} {req_course_code[4:]}\"]:\n",
    "                req_db_id = self.course_id_cache.get(possible_code)\n",
    "                if req_db_id:\n",
    "                    break\n",
    "            \n",
    "            if req_db_id:\n",
    "                self.cur.execute(\"\"\"\n",
    "                    INSERT INTO grade_requirements \n",
    "                    (course_id, required_course_id, minimum_grade)\n",
    "                    VALUES (%s, %s, %s)\n",
    "                    ON CONFLICT DO NOTHING\n",
    "                \"\"\", (course_db_id, req_db_id, min_grade))\n",
    "    \n",
    "    def commit(self):\n",
    "        \"\"\"Commit the current transaction.\"\"\"\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def rollback(self):\n",
    "        \"\"\"Rollback the current transaction.\"\"\"\n",
    "        self.conn.rollback()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        self.cur.close()\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac923bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: RequisiteParser Class\n",
    "class RequisiteParser:\n",
    "    def __init__(self, model=\"gemini-1.5-flash\", delay: float = 2.1):\n",
    "        self.model = genai.GenerativeModel(model)\n",
    "        self.delay = delay\n",
    "        self.api_calls = 0\n",
    "        self.failed_parses = []\n",
    "        self.last_call_time = 0\n",
    "\n",
    "    def parse_requisites(self, raw: str, course_id: str = None) -> dict:\n",
    "        if not raw or not raw.strip():\n",
    "            return {\"prerequisites\": [], \"corequisites\": [], \"grade_requirements\": {}, \"requisites_note\": None}\n",
    "        now = time.time(); elapsed = now - self.last_call_time\n",
    "        if elapsed < self.delay:\n",
    "            time.sleep(self.delay - elapsed)\n",
    "        self.last_call_time = time.time()\n",
    "        self.api_calls += 1\n",
    "\n",
    "        prompt = f\"\"\"Parse the following course requisite statement and return JSON:\n",
    "\n",
    "    {{\n",
    "      \"prerequisites\": [...],\n",
    "      \"corequisites\": [...],\n",
    "      \"grade_requirements\": {{/* course→grade */}},\n",
    "      \"requisites_note\": null\n",
    "    }}\n",
    "\n",
    "Requisite statement:\n",
    "{raw}\n",
    "\n",
    "Return ONLY the JSON.\"\"\"\n",
    "        try:\n",
    "            resp = self.model.generate_content(prompt)\n",
    "            txt = resp.text.strip()\n",
    "            txt = re.sub(r'^```json\\s*','',txt)\n",
    "            txt = re.sub(r'\\s*```$','',txt)\n",
    "            res = json.loads(txt)\n",
    "            return self._validate_result(res)\n",
    "        except Exception as e:\n",
    "            if course_id:\n",
    "                self.failed_parses.append((course_id,str(e)))\n",
    "            return self._fallback_parse(raw)\n",
    "\n",
    "    def _validate_result(self, res: dict) -> dict:\n",
    "        out = {\n",
    "            \"prerequisites\": [],\n",
    "            \"corequisites\": [],\n",
    "            \"grade_requirements\": res.get(\"grade_requirements\", {}),\n",
    "            \"requisites_note\": res.get(\"requisites_note\")\n",
    "        }\n",
    "        for key in [\"prerequisites\",\"corequisites\"]:\n",
    "            lst = res.get(key,[])\n",
    "            cleaned=[]\n",
    "            for item in lst:\n",
    "                if isinstance(item,list):\n",
    "                    cleaned.append(item)\n",
    "                elif isinstance(item,str):\n",
    "                    cleaned.append([item])\n",
    "            out[key]=cleaned\n",
    "        if not isinstance(out[\"grade_requirements\"],dict):\n",
    "            out[\"grade_requirements\"]={}\n",
    "        return out\n",
    "\n",
    "    def _fallback_parse(self, raw: str) -> dict:\n",
    "        pat = re.compile(r'\\b[A-Z]{2,5}\\s?\\d{2,3}[A-Z]?\\d?\\b')\n",
    "        codes = pat.findall(raw)\n",
    "        norm=[]\n",
    "        for c in codes:\n",
    "            if ' ' not in c:\n",
    "                c=re.sub(r'([A-Z]+)(\\d)',r'\\1 \\2',c)\n",
    "            norm.append(c)\n",
    "        prereqs=[[c] for c in norm]\n",
    "        grades={}\n",
    "        if 'C or better' in raw or 'grade of C' in raw:\n",
    "            for c in norm: grades[c.replace(' ','')] = 'C'\n",
    "        note=None\n",
    "        if 'permission' in raw.lower(): note=\"Permission of instructor may be required\"\n",
    "        return {\"prerequisites\":prereqs, \"corequisites\":[], \"grade_requirements\":grades, \"requisites_note\":note}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f48d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Scraping Functions\n",
    "def get_department_links(only=None):\n",
    "    resp = requests.get(COURSE_INDEX_URL)\n",
    "    soup = BeautifulSoup(resp.text,\"html.parser\")\n",
    "    idx = soup.find(\"div\",{\"id\":\"atozindex\"})\n",
    "    links=[]\n",
    "    for a in idx.find_all(\"a\",href=True):\n",
    "        code=a['href'].split(\"/\")[-2].upper()\n",
    "        if only is None or code in only:\n",
    "            links.append((code,urljoin(BASE_URL,a['href'])))\n",
    "    return links\n",
    "\n",
    "def parse_course_block(block, parser: RequisiteParser):\n",
    "    data={k:None for k in [\"department\",\"course_number\",\"course_id\",\"course_name\",\"credits\",\"description\"]}\n",
    "    data.update({\"requisites\":{\"prerequisites\":[],\"corequisites\":[]},\"grade_requirements\":{},\"requisites_note\":None,\"gen_ed\":None,\"grading_status\":None})\n",
    "    header=block.find(\"div\",class_=\"cols noindent\")\n",
    "    if header:\n",
    "        sts=header.find_all(\"strong\")\n",
    "        if len(sts)>=3:\n",
    "            code=sts[0].text.strip()\n",
    "            if \" \" in code:\n",
    "                d,n=code.split(\" \",1)\n",
    "                n=n.rstrip(\".\")\n",
    "                data[\"department\"],data[\"course_number\"]=d,n\n",
    "                data[\"course_id\"]=f\"{d} {n}\"\n",
    "            data[\"course_name\"]=sts[1].text.strip()\n",
    "            data[\"credits\"]=sts[2].text.strip().replace(\" Credits.\",\"\")\n",
    "    desc=block.find(\"p\",class_=\"courseblockextra\")\n",
    "    if desc: data[\"description\"]=desc.text.strip()\n",
    "    req=block.find(\"span\",class_=\"text detail-requisites margin--default\")\n",
    "    if req:\n",
    "        cid=data.get(\"course_id\",\"Unknown\")\n",
    "        rd=parser.parse_requisites(req.text,cid)\n",
    "        data[\"requisites\"]= {\"prerequisites\":rd[\"prerequisites\"],\"corequisites\":rd[\"corequisites\"]}\n",
    "        data[\"grade_requirements\"]=rd[\"grade_requirements\"]\n",
    "        data[\"requisites_note\"]=rd[\"requisites_note\"]\n",
    "    idea=block.find(\"span\",class_=\"text detail-idea_action margin--default\")\n",
    "    if idea: data[\"gen_ed\"]=idea.text.strip().replace(\"IDEAs in Action Gen Ed:\",\"\")\n",
    "    grade=block.find(\"span\",class_=\"text detail-grading_status margin--default\")\n",
    "    if grade: data[\"grading_status\"]=grade.text.strip().replace(\"Grading Status: \",\"\")\n",
    "    return data\n",
    "\n",
    "def parse_department(url, parser: RequisiteParser, dept_code: str, db_manager: DatabaseManager, mode: str = 'database'):\n",
    "    \"\"\"Parse all courses from a department page.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    course_blocks = soup.find_all(\"div\", class_=\"courseblock\")\n",
    "    \n",
    "    total_courses = len(course_blocks)\n",
    "    print(f\"   Found {total_courses} courses to parse\")\n",
    "    \n",
    "    courses = []\n",
    "    saved_count = 0\n",
    "    \n",
    "    for i, cb in enumerate(course_blocks, 1):\n",
    "        # Extract course ID for progress display\n",
    "        header = cb.find(\"div\", class_=\"cols noindent\")\n",
    "        course_id = \"Unknown\"\n",
    "        if header:\n",
    "            strong_tags = header.find_all(\"strong\")\n",
    "            if strong_tags:\n",
    "                course_id = strong_tags[0].text.strip()\n",
    "        \n",
    "        print(f\"   Processing {course_id} ({i}/{total_courses})...\", end='\\r')\n",
    "        \n",
    "        course_data = parse_course_block(cb, parser)\n",
    "        \n",
    "        # Save to database if in database mode\n",
    "        if mode in ['database', 'both'] and db_manager:\n",
    "            try:\n",
    "                db_manager.save_course(course_data)\n",
    "                saved_count += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to save {course_id}: {e}\")\n",
    "        \n",
    "        # Collect for JSON if needed\n",
    "        if mode in ['json', 'both']:\n",
    "            courses.append(course_data)\n",
    "    \n",
    "    print(f\"   ✓ Completed all {total_courses} courses in {dept_code} (saved {saved_count} to database)     \")\n",
    "    return courses\n",
    "\n",
    "def scrape_all_courses(parser: RequisiteParser, db_manager: Optional[DatabaseManager] = None, \n",
    "                      only=None, mode='database', dry_run=False, update_existing=True):\n",
    "    \"\"\"\n",
    "    Scrape all courses with flexible output options.\n",
    "    \n",
    "    Args:\n",
    "        parser: RequisiteParser instance\n",
    "        db_manager: DatabaseManager instance (required for database mode)\n",
    "        only: Set of department codes to scrape (None for all)\n",
    "        mode: 'database', 'json', or 'both'\n",
    "        dry_run: If True, don't actually save anything\n",
    "        update_existing: If True, update existing courses; if False, skip them\n",
    "    \"\"\"\n",
    "    department_links = get_department_links(only=only)\n",
    "    all_courses = {}\n",
    "    \n",
    "    print(f\"\\n🎯 Starting scrape of {len(department_links)} departments\")\n",
    "    print(f\"   Mode: {mode}\")\n",
    "    print(f\"   Dry run: {dry_run}\")\n",
    "    print(f\"   Update existing: {update_existing}\\n\")\n",
    "    \n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    for dept_idx, (dept_code, url) in enumerate(department_links, 1):\n",
    "        try:\n",
    "            print(f\"📚 [{dept_idx}/{len(department_links)}] Scraping {dept_code}...\")\n",
    "            dept_start_time = time.time()\n",
    "            \n",
    "            # Begin transaction for this department\n",
    "            if db_manager and not dry_run:\n",
    "                db_manager.conn.commit()  # Commit any pending changes\n",
    "            \n",
    "            courses = parse_department(url, parser, dept_code, db_manager if not dry_run else None, mode)\n",
    "            \n",
    "            if mode in ['json', 'both']:\n",
    "                all_courses[dept_code] = courses\n",
    "            \n",
    "            # Commit department transaction\n",
    "            if db_manager and not dry_run and mode in ['database', 'both']:\n",
    "                db_manager.commit()\n",
    "            \n",
    "            dept_elapsed = time.time() - dept_start_time\n",
    "            print(f\"✅ Successfully scraped {dept_code} in {dept_elapsed/60:.1f} minutes\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error scraping {dept_code}: {e}\\n\")\n",
    "            if db_manager and not dry_run:\n",
    "                db_manager.rollback()\n",
    "    \n",
    "    overall_elapsed = time.time() - overall_start_time\n",
    "    print(f\"⏱️  Total scraping time: {overall_elapsed/60:.1f} minutes\")\n",
    "    \n",
    "    return all_courses\n",
    "\n",
    "def save_to_json(data, filename=\"unc_courses.json\"):\n",
    "    \"\"\"Save course data to JSON file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\n💾 Saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fb77b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 12:33:18,055 - INFO - Loaded 1 departments and 123 courses into cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Starting scrape of 1 departments\n",
      "   Mode: database\n",
      "   Dry run: False\n",
      "   Update existing: True\n",
      "\n",
      "📚 [1/1] Scraping AERO...\n",
      "   Found 13 courses to parse\n",
      "   ✓ Completed all 13 courses in AERO (saved 13 to database)     \n",
      "✅ Successfully scraped AERO in 1.9 minutes\n",
      "\n",
      "⏱️  Total scraping time: 1.9 minutes\n",
      "\n",
      "📊 API calls: 4, Failed parses: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main Scrape & Save\n",
    "parser     = RequisiteParser(model=\"gemini-1.5-flash\", delay=0.5)\n",
    "db_manager = DatabaseManager(DATABASE_URL)\n",
    "\n",
    "MODE           = 'database'   # 'database', 'json', or 'both'\n",
    "DRY_RUN        = False\n",
    "UPDATE_EXISTING= True\n",
    "sample_depts   = {\"AAAD\", \"AERO\"}\n",
    "\n",
    "courses = scrape_all_courses(\n",
    "    parser,\n",
    "    db_manager,\n",
    "    only=sample_depts,\n",
    "    mode=MODE,\n",
    "    dry_run=DRY_RUN,\n",
    "    update_existing=UPDATE_EXISTING\n",
    ")\n",
    "\n",
    "if MODE in ['json','both']:\n",
    "    save_to_json(courses,\"unc_courses_sample.json\")\n",
    "\n",
    "print(f\"\\n📊 API calls: {parser.api_calls}, Failed parses: {len(parser.failed_parses)}\")\n",
    "db_manager.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28faaee3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m             prereqs = db.get_course_prerequisites(cid) \u001b[38;5;28;01mif\u001b[39;00m course \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m     10\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcourse[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mcourse\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - prereqs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prereqs.get(\u001b[33m'\u001b[39m\u001b[33mprerequisites\u001b[39m\u001b[33m'\u001b[39m,[]))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mverify_scraping_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mverify_scraping_results\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mverify_scraping_results\u001b[39m():\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb_queries\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CourseDatabase\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mCourseDatabase\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m db:\n\u001b[32m      5\u001b[39m         stats = db.get_database_stats()\n\u001b[32m      6\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔍 DB Stats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\malac\\pathfinder\\scraping\\db_queries.py:27\u001b[39m, in \u001b[36mCourseDatabase.__init__\u001b[39m\u001b[34m(self, db_url)\u001b[39m\n\u001b[32m     15\u001b[39m url = urlparse(db_url)\n\u001b[32m     17\u001b[39m conn_params = {\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m\"\u001b[39m: url.hostname,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m: url.port,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgssencmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdisable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28mself\u001b[39m.conn = \u001b[43mpsycopg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconn_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mself\u001b[39m.cur = \u001b[38;5;28mself\u001b[39m.conn.cursor(cursor_factory=RealDictCursor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\malac\\anaconda3\\envs\\pathfinder\\Lib\\site-packages\\psycopg2\\__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Verification / Lookup\n",
    "def verify_scraping_results():\n",
    "    from db_queries import CourseDatabase\n",
    "    with CourseDatabase() as db:\n",
    "        stats = db.get_database_stats()\n",
    "        print(f\"\\n🔍 DB Stats: {stats}\")\n",
    "        for cid in [\"COMP 110\",\"COMP 211\",\"BIOL 101\"]:\n",
    "            course = db.get_course(cid)\n",
    "            prereqs = db.get_course_prerequisites(cid) if course else {}\n",
    "            print(f\"{cid}: {course['name'] if course else 'N/A'} - prereqs: {len(prereqs.get('prerequisites',[]))}\")\n",
    "\n",
    "verify_scraping_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e465a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing 152 departments...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning departments: 100%|██████████| 152/152 [01:04<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Requisite Analysis Complete!\n",
      "\n",
      "Total courses across all departments: 10212\n",
      "Courses with requisites: 2759\n",
      "Courses without requisites: 7453\n",
      "Percentage with requisites: 27.0%\n",
      "\n",
      "💡 You will need 2759 API calls\n",
      "⏱️  Estimated time at 2.1s/call: 96.6 minutes\n",
      "\n",
      "📈 Top 10 departments by requisite count:\n",
      "   BIOL: 167/264 courses (63%)\n",
      "   PSYC: 101/183 courses (55%)\n",
      "   NURS: 91/169 courses (54%)\n",
      "   ECON: 81/145 courses (56%)\n",
      "   COMP: 79/109 courses (72%)\n",
      "   PHCY: 75/112 courses (67%)\n",
      "   CHEM: 74/112 courses (66%)\n",
      "   SPAN: 71/119 courses (60%)\n",
      "   MATH: 70/110 courses (64%)\n",
      "   COMM: 69/213 courses (32%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell to count non-empty requisites across all departments\n",
    "def count_requisites(only=None):\n",
    "    \"\"\"Count how many courses have non-empty requisites across departments.\"\"\"\n",
    "    department_links = get_department_links(only=only)\n",
    "    \n",
    "    total_courses = 0\n",
    "    courses_with_requisites = 0\n",
    "    dept_stats = {}\n",
    "    \n",
    "    print(f\"🔍 Analyzing {len(department_links)} departments...\\n\")\n",
    "    \n",
    "    for dept_code, url in tqdm(department_links, desc=\"Scanning departments\"):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        course_blocks = soup.find_all(\"div\", class_=\"courseblock\")\n",
    "        \n",
    "        dept_total = len(course_blocks)\n",
    "        dept_with_reqs = 0\n",
    "        \n",
    "        for block in course_blocks:\n",
    "            req_span = block.find(\"span\", class_=\"text detail-requisites margin--default\")\n",
    "            if req_span and req_span.text.strip() and req_span.text.strip() != \"Requisites:\":\n",
    "                dept_with_reqs += 1\n",
    "        \n",
    "        dept_stats[dept_code] = {\n",
    "            \"total\": dept_total,\n",
    "            \"with_requisites\": dept_with_reqs,\n",
    "            \"percentage\": (dept_with_reqs / dept_total * 100) if dept_total > 0 else 0\n",
    "        }\n",
    "        \n",
    "        total_courses += dept_total\n",
    "        courses_with_requisites += dept_with_reqs\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n📊 Requisite Analysis Complete!\\n\")\n",
    "    print(f\"Total courses across all departments: {total_courses}\")\n",
    "    print(f\"Courses with requisites: {courses_with_requisites}\")\n",
    "    print(f\"Courses without requisites: {total_courses - courses_with_requisites}\")\n",
    "    print(f\"Percentage with requisites: {courses_with_requisites/total_courses*100:.1f}%\")\n",
    "    print(f\"\\n💡 You will need {courses_with_requisites} API calls\")\n",
    "    print(f\"⏱️  Estimated time at 2.1s/call: {courses_with_requisites * 2.1 / 60:.1f} minutes\")\n",
    "    \n",
    "    # Show top departments by requisite count\n",
    "    print(f\"\\n📈 Top 10 departments by requisite count:\")\n",
    "    sorted_depts = sorted(dept_stats.items(), key=lambda x: x[1]['with_requisites'], reverse=True)[:10]\n",
    "    for dept, stats in sorted_depts:\n",
    "        print(f\"   {dept}: {stats['with_requisites']}/{stats['total']} courses ({stats['percentage']:.0f}%)\")\n",
    "    \n",
    "    return dept_stats\n",
    "\n",
    "# Run the analysis\n",
    "# For all departments:\n",
    "dept_stats = count_requisites()\n",
    "\n",
    "# Or for specific departments:\n",
    "# dept_stats = count_requisites(only={\"COMP\", \"MATH\", \"BIOL\", \"CHEM\", \"PHYS\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
