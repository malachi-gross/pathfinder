{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c3a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malac\\anaconda3\\envs\\pathfinder\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import and setup\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor, Json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344947c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configure API and Database\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
    "\n",
    "if not DATABASE_URL:\n",
    "    raise ValueError(\"DATABASE_URL not found in .env file\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "BASE_URL = \"https://catalog.unc.edu\"\n",
    "COURSE_INDEX_URL = f\"{BASE_URL}/courses/#text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c1151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Database Manager class\n",
    "class DatabaseManager:\n",
    "    def __init__(self, db_url: str):\n",
    "        \"\"\"Initialize database connection and caches.\"\"\"\n",
    "        # Parse the URL to add gssencmode parameter\n",
    "        from urllib.parse import urlparse\n",
    "        url = urlparse(db_url)\n",
    "        \n",
    "        conn_params = {\n",
    "            \"host\": url.hostname,\n",
    "            \"port\": url.port,\n",
    "            \"database\": url.path[1:],  # Remove leading '/'\n",
    "            \"user\": url.username,\n",
    "            \"password\": url.password,\n",
    "            \"sslmode\": \"require\",\n",
    "            \"gssencmode\": \"disable\"  # Fix for Windows GSSAPI error\n",
    "        }\n",
    "        \n",
    "        self.conn = psycopg2.connect(**conn_params)\n",
    "        self.conn.autocommit = False\n",
    "        self.cur = self.conn.cursor(cursor_factory=RealDictCursor)\n",
    "        \n",
    "        # Cache for lookups\n",
    "        self.department_cache = {}\n",
    "        self.course_id_cache = {}\n",
    "        \n",
    "        # Load existing data into cache\n",
    "        self._load_cache()\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        \"\"\"Load existing departments and courses into cache.\"\"\"\n",
    "        # Load departments\n",
    "        self.cur.execute(\"SELECT id, code FROM departments\")\n",
    "        for row in self.cur.fetchall():\n",
    "            self.department_cache[row['code']] = row['id']\n",
    "        \n",
    "        # Load course IDs\n",
    "        self.cur.execute(\"SELECT id, course_id FROM courses\")\n",
    "        for row in self.cur.fetchall():\n",
    "            self.course_id_cache[row['course_id']] = row['id']\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.department_cache)} departments and {len(self.course_id_cache)} courses into cache\")\n",
    "    \n",
    "    def get_or_create_department(self, dept_code: str) -> int:\n",
    "        \"\"\"Get or create a department, returning its ID.\"\"\"\n",
    "        if dept_code in self.department_cache:\n",
    "            return self.department_cache[dept_code]\n",
    "        \n",
    "        self.cur.execute(\"\"\"\n",
    "            INSERT INTO departments (code) \n",
    "            VALUES (%s) \n",
    "            ON CONFLICT (code) DO UPDATE SET code = EXCLUDED.code\n",
    "            RETURNING id\n",
    "        \"\"\", (dept_code,))\n",
    "        \n",
    "        dept_id = self.cur.fetchone()['id']\n",
    "        self.department_cache[dept_code] = dept_id\n",
    "        return dept_id\n",
    "    \n",
    "    def save_course(self, course_data: Dict) -> Optional[int]:\n",
    "        \"\"\"Save a course to the database.\"\"\"\n",
    "        try:\n",
    "            dept_id = self.get_or_create_department(course_data['department'])\n",
    "            \n",
    "            # Extract gen_ed as array\n",
    "            gen_ed = []\n",
    "            if course_data.get('gen_ed'):\n",
    "                gen_ed = [course_data['gen_ed']] if isinstance(course_data['gen_ed'], str) else course_data['gen_ed']\n",
    "            \n",
    "            # Insert or update course\n",
    "            self.cur.execute(\"\"\"\n",
    "                INSERT INTO courses \n",
    "                (course_id, department_id, course_number, name, description, \n",
    "                 credits, gen_ed, grading_status)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (course_id) DO UPDATE SET\n",
    "                    name = EXCLUDED.name,\n",
    "                    description = EXCLUDED.description,\n",
    "                    credits = EXCLUDED.credits,\n",
    "                    gen_ed = EXCLUDED.gen_ed,\n",
    "                    grading_status = EXCLUDED.grading_status,\n",
    "                    updated_at = NOW()\n",
    "                RETURNING id\n",
    "            \"\"\", (\n",
    "                course_data['course_id'],\n",
    "                dept_id,\n",
    "                course_data['course_number'],\n",
    "                course_data['course_name'],\n",
    "                course_data.get('description'),\n",
    "                course_data.get('credits'),\n",
    "                gen_ed,\n",
    "                course_data.get('grading_status')\n",
    "            ))\n",
    "            \n",
    "            course_db_id = self.cur.fetchone()['id']\n",
    "            self.course_id_cache[course_data['course_id']] = course_db_id\n",
    "            \n",
    "            # Save prerequisites if present\n",
    "            if course_data.get('requisites'):\n",
    "                self._save_prerequisites(course_db_id, course_data['requisites'])\n",
    "            \n",
    "            # Save grade requirements if present\n",
    "            if course_data.get('grade_requirements'):\n",
    "                self._save_grade_requirements(course_db_id, course_data['grade_requirements'])\n",
    "            \n",
    "            return course_db_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving course {course_data.get('course_id')}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _save_prerequisites(self, course_db_id: int, requisites: Dict):\n",
    "        \"\"\"Save prerequisites for a course.\"\"\"\n",
    "        # Clear existing prerequisites\n",
    "        self.cur.execute(\"DELETE FROM prerequisites WHERE course_id = %s\", (course_db_id,))\n",
    "        \n",
    "        # Save prerequisites (AND groups)\n",
    "        for group_idx, prereq_group in enumerate(requisites.get('prerequisites', [])):\n",
    "            for prereq_course_code in prereq_group:\n",
    "                prereq_db_id = self.course_id_cache.get(prereq_course_code.strip())\n",
    "                if prereq_db_id:\n",
    "                    self.cur.execute(\"\"\"\n",
    "                        INSERT INTO prerequisites \n",
    "                        (course_id, prereq_group, prereq_course_id, is_corequisite)\n",
    "                        VALUES (%s, %s, %s, %s)\n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", (course_db_id, group_idx, prereq_db_id, False))\n",
    "        \n",
    "        # Save corequisites\n",
    "        for group_idx, coreq_group in enumerate(requisites.get('corequisites', [])):\n",
    "            for coreq_course_code in coreq_group:\n",
    "                coreq_db_id = self.course_id_cache.get(coreq_course_code.strip())\n",
    "                if coreq_db_id:\n",
    "                    self.cur.execute(\"\"\"\n",
    "                        INSERT INTO prerequisites \n",
    "                        (course_id, prereq_group, prereq_course_id, is_corequisite)\n",
    "                        VALUES (%s, %s, %s, %s)\n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", (course_db_id, group_idx + 1000, coreq_db_id, True))\n",
    "    \n",
    "    def _save_grade_requirements(self, course_db_id: int, grade_requirements: Dict):\n",
    "        \"\"\"Save grade requirements for a course.\"\"\"\n",
    "        # Clear existing grade requirements\n",
    "        self.cur.execute(\"DELETE FROM grade_requirements WHERE course_id = %s\", (course_db_id,))\n",
    "        \n",
    "        for req_course_code, min_grade in grade_requirements.items():\n",
    "            # Try different formats\n",
    "            req_course_code = req_course_code.replace(' ', '')\n",
    "            req_db_id = None\n",
    "            for possible_code in [req_course_code, f\"{req_course_code[:4]} {req_course_code[4:]}\"]:\n",
    "                req_db_id = self.course_id_cache.get(possible_code)\n",
    "                if req_db_id:\n",
    "                    break\n",
    "            \n",
    "            if req_db_id:\n",
    "                self.cur.execute(\"\"\"\n",
    "                    INSERT INTO grade_requirements \n",
    "                    (course_id, required_course_id, minimum_grade)\n",
    "                    VALUES (%s, %s, %s)\n",
    "                    ON CONFLICT DO NOTHING\n",
    "                \"\"\", (course_db_id, req_db_id, min_grade))\n",
    "    \n",
    "    def commit(self):\n",
    "        \"\"\"Commit the current transaction.\"\"\"\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def rollback(self):\n",
    "        \"\"\"Rollback the current transaction.\"\"\"\n",
    "        self.conn.rollback()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        self.cur.close()\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac923bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: RequisiteParser class\n",
    "class RequisiteParser:\n",
    "    def __init__(self, model=\"gemini-2.5-flash\", delay: float = 2.1):\n",
    "        \"\"\"Initialize the parser with Gemini API.\"\"\"\n",
    "        self.model = genai.GenerativeModel(model)\n",
    "        self.delay = delay\n",
    "        self.api_calls = 0\n",
    "        self.failed_parses = []\n",
    "        self.last_call_time = 0\n",
    "        \n",
    "    def parse_requisites(self, raw: str, course_id: str = None) -> dict:\n",
    "        \"\"\"Parse requisites using Gemini API.\"\"\"\n",
    "        if not raw or not raw.strip():\n",
    "            return {\n",
    "                \"prerequisites\": [],\n",
    "                \"corequisites\": [],\n",
    "                \"grade_requirements\": {},\n",
    "                \"requisites_note\": None\n",
    "            }\n",
    "        \n",
    "        # Rate limiting\n",
    "        current_time = time.time()\n",
    "        time_since_last_call = current_time - self.last_call_time\n",
    "        if time_since_last_call < self.delay:\n",
    "            sleep_time = self.delay - time_since_last_call\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_call_time = time.time()\n",
    "        self.api_calls += 1\n",
    "        \n",
    "        prompt = f\"\"\"Parse the following course requisite statement and return a JSON object with this exact structure:\n",
    "\n",
    "{{\n",
    "    \"prerequisites\": [\n",
    "        // List of AND-groups, where each group is a list of courses that can be taken as alternatives (OR)\n",
    "        // Example: [[\"COMP 110\"], [\"MATH 231\", \"MATH 241\"]] means COMP 110 AND (MATH 231 OR MATH 241)\n",
    "    ],\n",
    "    \"corequisites\": [\n",
    "        // Same structure as prerequisites but for co-requisites\n",
    "    ],\n",
    "    \"grade_requirements\": {{\n",
    "        // Map of course to required grade\n",
    "        // Example: {{\"COMP 110\": \"C\", \"MATH 231\": \"C+\"}}\n",
    "    }},\n",
    "    \"requisites_note\": // String with any additional requirements like \"permission of instructor\" or null if none\n",
    "}}\n",
    "\n",
    "CRITICAL PARSING RULES:\n",
    "\n",
    "1. AND relationships (all required):\n",
    "   - Separated by \"and\", semicolons (;), or commas in a list\n",
    "   - Example: \"COMP 110 and MATH 231\" → [[\"COMP 110\"], [\"MATH 231\"]]\n",
    "   - Example: \"COMP 210; COMP 211; COMP 301\" → [[\"COMP 210\"], [\"COMP 211\"], [\"COMP 301\"]]\n",
    "\n",
    "2. OR relationships (choose one):\n",
    "   - Separated by \"or\"\n",
    "   - Example: \"COMP 283 or MATH 381 or STOR 315\" → [[\"COMP 283\", \"MATH 381\", \"STOR 315\"]]\n",
    "\n",
    "3. Mixed AND/OR:\n",
    "   - Example: \"MATH 231 or 241; COMP 210, COMP 211, and COMP 301\"\n",
    "   - Parse as: [[\"MATH 231\", \"MATH 241\"], [\"COMP 210\"], [\"COMP 211\"], [\"COMP 301\"]]\n",
    "   - The semicolon separates AND groups, \"or\" creates OR options within a group\n",
    "\n",
    "4. Pre- or corequisites:\n",
    "   - Add the SAME courses to BOTH prerequisites and corequisites arrays\n",
    "   - Example: \"Pre- or corequisites, COMP 283 or MATH 381\"\n",
    "   - Prerequisites: [[\"COMP 283\", \"MATH 381\"]]\n",
    "   - Corequisites: [[\"COMP 283\", \"MATH 381\"]]\n",
    "\n",
    "5. Grade requirements:\n",
    "   - Look for \"grade of X or better\", \"C or better\", etc.\n",
    "   - Apply to ALL courses mentioned in the same clause\n",
    "   - Example: \"COMP 211 and COMP 301; a grade of C or better is required in both\"\n",
    "   - grade_requirements: {{\"COMP 211\": \"C\", \"COMP 301\": \"C\"}}\n",
    "\n",
    "6. Course code format:\n",
    "   - Always format as \"DEPT ###\" with a space (e.g., \"COMP 110\", not \"COMP110\")\n",
    "   - Include letter suffixes if present (e.g., \"BIOL 101L\")\n",
    "\n",
    "7. Special requirements (if applicable, put in requisites_note):\n",
    "   - \"Permission of the instructor\" → Include exact text\n",
    "   - \"May be repeated for credit\" → Include this note\n",
    "   - \"Not open to students who have credit for X\" → Include full restriction\n",
    "   - \"for students lacking the prerequisite\" → Include context\n",
    "   - Any GPA requirements → Include exact GPA needed\n",
    "   - Class standing restrictions (e.g., \"Juniors and seniors only\")\n",
    "\n",
    "Common patterns to recognize:\n",
    "- \"Prerequisites, X and Y\" → both required\n",
    "- \"Prerequisite, X or Y\" → choose one\n",
    "- \"Prerequisites, X; Y or Z\" → X is required AND (Y OR Z)\n",
    "- \"one of the following\" → all listed courses are OR options\n",
    "- \"all of the following\" → all listed courses are AND requirements\n",
    "- \"permission of the instructor for students lacking the prerequisite\" → courses are still required, but add note about permission option\n",
    "\n",
    "Example complex requisite:\n",
    "\"Prerequisites, COMP 211 and 301, or COMP 401, 410, and 411; a grade of C or better is required in all prerequisite courses; permission of the instructor for students lacking the prerequisites; may be repeated for credit.\"\n",
    "\n",
    "Should parse to:\n",
    "{{\n",
    "    \"prerequisites\": [[\"COMP 211\", \"COMP 301\"], [\"COMP 401\", \"COMP 410\", \"COMP 411\"]],\n",
    "    \"corequisites\": [],\n",
    "    \"grade_requirements\": {{\"COMP 211\": \"C\", \"COMP 301\": \"C\", \"COMP 401\": \"C\", \"COMP 410\": \"C\", \"COMP 411\": \"C\"}},\n",
    "    \"requisites_note\": \"permission of the instructor for students lacking the prerequisites; may be repeated for credit\"\n",
    "}}\n",
    "\n",
    "Requisite statement to parse:\n",
    "{raw}\n",
    "\n",
    "Return ONLY the JSON object, no explanation or markdown.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            json_text = response.text.strip()\n",
    "            json_text = re.sub(r'^```json\\s*', '', json_text)\n",
    "            json_text = re.sub(r'\\s*```$', '', json_text)\n",
    "            \n",
    "            result = json.loads(json_text)\n",
    "            return self._validate_result(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if course_id:\n",
    "                self.failed_parses.append((course_id, str(e)))\n",
    "            return self._fallback_parse(raw)\n",
    "    \n",
    "    def _validate_result(self, result: dict) -> dict:\n",
    "        \"\"\"Validate and clean the parsed result.\"\"\"\n",
    "        validated = {\n",
    "            \"prerequisites\": result.get(\"prerequisites\", []),\n",
    "            \"corequisites\": result.get(\"corequisites\", []),\n",
    "            \"grade_requirements\": result.get(\"grade_requirements\", {}),\n",
    "            \"requisites_note\": result.get(\"requisites_note\", None)\n",
    "        }\n",
    "        \n",
    "        # Ensure prerequisites and corequisites are lists of lists\n",
    "        for key in [\"prerequisites\", \"corequisites\"]:\n",
    "            if not isinstance(validated[key], list):\n",
    "                validated[key] = []\n",
    "            else:\n",
    "                cleaned_list = []\n",
    "                for item in validated[key]:\n",
    "                    if isinstance(item, list):\n",
    "                        cleaned_list.append(item)\n",
    "                    elif isinstance(item, str):\n",
    "                        cleaned_list.append([item])\n",
    "                validated[key] = cleaned_list\n",
    "        \n",
    "        if not isinstance(validated[\"grade_requirements\"], dict):\n",
    "            validated[\"grade_requirements\"] = {}\n",
    "        \n",
    "        return validated\n",
    "    \n",
    "    def _fallback_parse(self, raw: str) -> dict:\n",
    "        \"\"\"Basic fallback parser if API fails.\"\"\"\n",
    "        course_pattern = re.compile(r'\\b[A-Z]{2,5}\\s?\\d{2,3}[A-Z]?\\d?[A-Z]?\\b')\n",
    "        courses = course_pattern.findall(raw)\n",
    "        \n",
    "        normalized_courses = []\n",
    "        for course in courses:\n",
    "            if ' ' not in course:\n",
    "                course = re.sub(r'([A-Z]+)(\\d)', r'\\1 \\2', course)\n",
    "            normalized_courses.append(course)\n",
    "        \n",
    "        prerequisites = [[course] for course in normalized_courses]\n",
    "        \n",
    "        grade_requirements = {}\n",
    "        if 'C or better' in raw or 'grade of C' in raw:\n",
    "            for course in normalized_courses:\n",
    "                grade_requirements[course.replace(' ', '')] = 'C'\n",
    "        \n",
    "        note = None\n",
    "        if 'permission' in raw.lower() or 'instructor' in raw.lower():\n",
    "            note = \"Permission of instructor may be required\"\n",
    "        \n",
    "        return {\n",
    "            \"prerequisites\": prerequisites,\n",
    "            \"corequisites\": [],\n",
    "            \"grade_requirements\": grade_requirements,\n",
    "            \"requisites_note\": note\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f48d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Scraping functions\n",
    "def get_department_links(only=None):\n",
    "    \"\"\"Scrape all department links from the main courses page.\"\"\"\n",
    "    response = requests.get(COURSE_INDEX_URL)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    index_div = soup.find(\"div\", {\"id\": \"atozindex\"})\n",
    "    links = []\n",
    "\n",
    "    for a in index_div.find_all(\"a\", href=True):\n",
    "        dept_code = a['href'].split(\"/\")[-2].upper()\n",
    "        if only is None or dept_code in only:\n",
    "            links.append((dept_code, urljoin(BASE_URL, a['href'])))\n",
    "\n",
    "    return links\n",
    "\n",
    "def parse_course_block(block, parser: RequisiteParser):\n",
    "    \"\"\"Parse a course block from the HTML.\"\"\"\n",
    "    data = {\n",
    "        \"department\": None,\n",
    "        \"course_number\": None,\n",
    "        \"course_name\": None,\n",
    "        \"credits\": None,\n",
    "        \"description\": None,\n",
    "        \"requisites\": {\"prerequisites\": [], \"corequisites\": []},\n",
    "        \"grade_requirements\": {},\n",
    "        \"requisites_note\": None,\n",
    "        \"gen_ed\": None,\n",
    "        \"grading_status\": None\n",
    "    }\n",
    "\n",
    "    # Header line\n",
    "    header = block.find(\"div\", class_=\"cols noindent\")\n",
    "    if header:\n",
    "        strong_tags = header.find_all(\"strong\")\n",
    "        if len(strong_tags) >= 3:\n",
    "            code = strong_tags[0].text.strip()\n",
    "            if \" \" in code:\n",
    "                data[\"department\"], data[\"course_number\"] = code.split(\" \", 1)\n",
    "                data[\"course_number\"] = data[\"course_number\"].rstrip(\".\")\n",
    "            data[\"course_id\"] = f\"{data['department']} {data['course_number']}\"\n",
    "            data[\"course_name\"] = strong_tags[1].text.strip()\n",
    "            data[\"credits\"] = strong_tags[2].text.strip().replace(\" Credits.\", \"\")\n",
    "\n",
    "    # Description\n",
    "    desc_block = block.find(\"p\", class_=\"courseblockextra\")\n",
    "    if desc_block:\n",
    "        data[\"description\"] = desc_block.text.strip()\n",
    "\n",
    "    # Requisites - using LLM parser\n",
    "    req_span = block.find(\"span\", class_=\"text detail-requisites margin--default\")\n",
    "    if req_span:\n",
    "        course_id = data.get(\"course_id\", \"Unknown\")\n",
    "        req_data = parser.parse_requisites(req_span.text, course_id)\n",
    "        data[\"requisites\"] = {\n",
    "            \"prerequisites\": req_data[\"prerequisites\"],\n",
    "            \"corequisites\": req_data[\"corequisites\"]\n",
    "        }\n",
    "        data[\"grade_requirements\"] = req_data[\"grade_requirements\"]\n",
    "        data[\"requisites_note\"] = req_data[\"requisites_note\"]\n",
    "\n",
    "    # Gen Ed\n",
    "    idea_span = block.find(\"span\", class_=\"text detail-idea_action margin--default\")\n",
    "    if idea_span:\n",
    "        idea_text = idea_span.text.strip().replace(\"IDEAs in Action Gen Ed:\", \"\")\n",
    "        data[\"gen_ed\"] = idea_text.strip()\n",
    "\n",
    "    # Grading\n",
    "    grading_span = block.find(\"span\", class_=\"text detail-grading_status margin--default\")\n",
    "    if grading_span:\n",
    "        data[\"grading_status\"] = grading_span.text.strip().replace(\"Grading Status: \", \"\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_department(url, parser: RequisiteParser, dept_code: str, db_manager: DatabaseManager, mode: str = 'database'):\n",
    "    \"\"\"Parse all courses from a department page.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    course_blocks = soup.find_all(\"div\", class_=\"courseblock\")\n",
    "    \n",
    "    total_courses = len(course_blocks)\n",
    "    print(f\"   Found {total_courses} courses to parse\")\n",
    "    \n",
    "    courses = []\n",
    "    saved_count = 0\n",
    "    \n",
    "    for i, cb in enumerate(course_blocks, 1):\n",
    "        # Extract course ID for progress display\n",
    "        header = cb.find(\"div\", class_=\"cols noindent\")\n",
    "        course_id = \"Unknown\"\n",
    "        if header:\n",
    "            strong_tags = header.find_all(\"strong\")\n",
    "            if strong_tags:\n",
    "                course_id = strong_tags[0].text.strip()\n",
    "        \n",
    "        print(f\"   Processing {course_id} ({i}/{total_courses})...\", end='\\r')\n",
    "        \n",
    "        course_data = parse_course_block(cb, parser)\n",
    "        \n",
    "        # Save to database if in database mode\n",
    "        if mode in ['database', 'both'] and db_manager:\n",
    "            try:\n",
    "                db_manager.save_course(course_data)\n",
    "                saved_count += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to save {course_id}: {e}\")\n",
    "        \n",
    "        # Collect for JSON if needed\n",
    "        if mode in ['json', 'both']:\n",
    "            courses.append(course_data)\n",
    "    \n",
    "    print(f\"   ✓ Completed all {total_courses} courses in {dept_code} (saved {saved_count} to database)     \")\n",
    "    return courses\n",
    "\n",
    "def scrape_all_courses(parser: RequisiteParser, db_manager: Optional[DatabaseManager] = None, \n",
    "                      only=None, mode='database', dry_run=False, update_existing=True):\n",
    "    \"\"\"\n",
    "    Scrape all courses with flexible output options.\n",
    "    \n",
    "    Args:\n",
    "        parser: RequisiteParser instance\n",
    "        db_manager: DatabaseManager instance (required for database mode)\n",
    "        only: Set of department codes to scrape (None for all)\n",
    "        mode: 'database', 'json', or 'both'\n",
    "        dry_run: If True, don't actually save anything\n",
    "        update_existing: If True, update existing courses; if False, skip them\n",
    "    \"\"\"\n",
    "    department_links = get_department_links(only=only)\n",
    "    all_courses = {}\n",
    "    \n",
    "    print(f\"\\n🎯 Starting scrape of {len(department_links)} departments\")\n",
    "    print(f\"   Mode: {mode}\")\n",
    "    print(f\"   Dry run: {dry_run}\")\n",
    "    print(f\"   Update existing: {update_existing}\\n\")\n",
    "    \n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    for dept_idx, (dept_code, url) in enumerate(department_links, 1):\n",
    "        try:\n",
    "            print(f\"📚 [{dept_idx}/{len(department_links)}] Scraping {dept_code}...\")\n",
    "            dept_start_time = time.time()\n",
    "            \n",
    "            # Begin transaction for this department\n",
    "            if db_manager and not dry_run:\n",
    "                db_manager.conn.commit()  # Commit any pending changes\n",
    "            \n",
    "            courses = parse_department(url, parser, dept_code, db_manager if not dry_run else None, mode)\n",
    "            \n",
    "            if mode in ['json', 'both']:\n",
    "                all_courses[dept_code] = courses\n",
    "            \n",
    "            # Commit department transaction\n",
    "            if db_manager and not dry_run and mode in ['database', 'both']:\n",
    "                db_manager.commit()\n",
    "            \n",
    "            dept_elapsed = time.time() - dept_start_time\n",
    "            print(f\"✅ Successfully scraped {dept_code} in {dept_elapsed/60:.1f} minutes\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error scraping {dept_code}: {e}\\n\")\n",
    "            if db_manager and not dry_run:\n",
    "                db_manager.rollback()\n",
    "    \n",
    "    overall_elapsed = time.time() - overall_start_time\n",
    "    print(f\"⏱️  Total scraping time: {overall_elapsed/60:.1f} minutes\")\n",
    "    \n",
    "    return all_courses\n",
    "\n",
    "def save_to_json(data, filename=\"unc_courses.json\"):\n",
    "    \"\"\"Save course data to JSON file.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\n💾 Saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fb77b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 11:55:57,473 - INFO - Loaded 7 departments and 564 courses into cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Starting scrape of 152 departments\n",
      "   Mode: database\n",
      "   Dry run: False\n",
      "   Update existing: True\n",
      "\n",
      "📚 [1/152] Scraping AERO...\n",
      "   Found 13 courses to parse\n",
      "   ✓ Completed all 13 courses in AERO (saved 13 to database)     \n",
      "✅ Successfully scraped AERO in 0.1 minutes\n",
      "\n",
      "📚 [2/152] Scraping AAAD...\n",
      "   Found 123 courses to parse\n",
      "   ✓ Completed all 123 courses in AAAD (saved 123 to database)     \n",
      "✅ Successfully scraped AAAD in 0.2 minutes\n",
      "\n",
      "📚 [3/152] Scraping AMST...\n",
      "   Found 108 courses to parse\n",
      "   ✓ Completed all 108 courses in AMST (saved 108 to database)     \n",
      "✅ Successfully scraped AMST in 0.2 minutes\n",
      "\n",
      "📚 [4/152] Scraping ANTH...\n",
      "   Found 253 courses to parse\n",
      "   ✓ Completed all 253 courses in ANTH (saved 253 to database)     \n",
      "✅ Successfully scraped ANTH in 0.5 minutes\n",
      "\n",
      "📚 [5/152] Scraping APPL...\n",
      "   Found 38 courses to parse\n",
      "   ✓ Completed all 38 courses in APPL (saved 38 to database)     \n",
      "✅ Successfully scraped APPL in 0.2 minutes\n",
      "\n",
      "📚 [6/152] Scraping ARAB...\n",
      "   Found 24 courses to parse\n",
      "   ✓ Completed all 24 courses in ARAB (saved 24 to database)     \n",
      "✅ Successfully scraped ARAB in 0.1 minutes\n",
      "\n",
      "📚 [7/152] Scraping ARCH...\n",
      "   Found 5 courses to parse\n",
      "   ✓ Completed all 5 courses in ARCH (saved 5 to database)     \n",
      "✅ Successfully scraped ARCH in 0.0 minutes\n",
      "\n",
      "📚 [8/152] Scraping ARMY...\n",
      "   Found 12 courses to parse\n",
      "   ✓ Completed all 12 courses in ARMY (saved 12 to database)     \n",
      "✅ Successfully scraped ARMY in 0.1 minutes\n",
      "\n",
      "📚 [9/152] Scraping ARTH...\n",
      "   Found 148 courses to parse\n",
      "   ✓ Completed all 148 courses in ARTH (saved 148 to database)     \n",
      "✅ Successfully scraped ARTH in 0.3 minutes\n",
      "\n",
      "📚 [10/152] Scraping ASIA...\n",
      "   Found 157 courses to parse\n",
      "   ✓ Completed all 157 courses in ASIA (saved 157 to database)     \n",
      "✅ Successfully scraped ASIA in 0.3 minutes\n",
      "\n",
      "📚 [11/152] Scraping ASTR...\n",
      "   Found 32 courses to parse\n",
      "   ✓ Completed all 32 courses in ASTR (saved 32 to database)     \n",
      "✅ Successfully scraped ASTR in 0.2 minutes\n",
      "\n",
      "📚 [12/152] Scraping BIOC...\n",
      "   Found 68 courses to parse\n",
      "   ✓ Completed all 68 courses in BIOC (saved 68 to database)     \n",
      "✅ Successfully scraped BIOC in 0.3 minutes\n",
      "\n",
      "📚 [13/152] Scraping BCB...\n",
      "   Found 32 courses to parse\n",
      "   ✓ Completed all 32 courses in BCB (saved 32 to database)     \n",
      "✅ Successfully scraped BCB in 0.1 minutes\n",
      "\n",
      "📚 [14/152] Scraping BBSP...\n",
      "   Found 7 courses to parse\n",
      "   ✓ Completed all 7 courses in BBSP (saved 7 to database)     \n",
      "✅ Successfully scraped BBSP in 0.0 minutes\n",
      "\n",
      "📚 [15/152] Scraping BIOL...\n",
      "   Found 264 courses to parse\n",
      "   ✓ Completed all 264 courses in BIOL (saved 264 to database)     \n",
      "✅ Successfully scraped BIOL in 1.9 minutes\n",
      "\n",
      "📚 [16/152] Scraping BMME...\n",
      "   Found 69 courses to parse\n",
      "   ✓ Completed all 69 courses in BMME (saved 69 to database)     \n",
      "✅ Successfully scraped BMME in 0.5 minutes\n",
      "\n",
      "📚 [17/152] Scraping BIOS...\n",
      "   Found 64 courses to parse\n",
      "   ✓ Completed all 64 courses in BIOS (saved 64 to database)     \n",
      "✅ Successfully scraped BIOS in 0.4 minutes\n",
      "\n",
      "📚 [18/152] Scraping BCS...\n",
      "   Found 8 courses to parse\n",
      "   ✓ Completed all 8 courses in BCS (saved 8 to database)     \n",
      "✅ Successfully scraped BCS in 0.1 minutes\n",
      "\n",
      "📚 [19/152] Scraping BUSI...\n",
      "   Found 178 courses to parse\n",
      "   ✓ Completed all 178 courses in BUSI (saved 178 to database)     \n",
      "✅ Successfully scraped BUSI in 0.7 minutes\n",
      "\n",
      "📚 [20/152] Scraping CHIP...\n",
      "   Found 12 courses to parse\n",
      "   ✓ Completed all 12 courses in CHIP (saved 12 to database)     \n",
      "✅ Successfully scraped CHIP in 0.0 minutes\n",
      "\n",
      "📚 [21/152] Scraping CATA...\n",
      "   Found 2 courses to parse\n",
      "   ✓ Completed all 2 courses in CATA (saved 2 to database)     \n",
      "✅ Successfully scraped CATA in 0.0 minutes\n",
      "\n",
      "📚 [22/152] Scraping CBPH...\n",
      "   Found 25 courses to parse\n",
      "   ✓ Completed all 25 courses in CBPH (saved 25 to database)     \n",
      "✅ Successfully scraped CBPH in 0.1 minutes\n",
      "\n",
      "📚 [23/152] Scraping CBMC...\n",
      "   Found 4 courses to parse\n",
      "   ✓ Completed all 4 courses in CBMC (saved 4 to database)     \n",
      "✅ Successfully scraped CBMC in 0.0 minutes\n",
      "\n",
      "📚 [24/152] Scraping CHEM...\n",
      "   Found 112 courses to parse\n",
      "   ✓ Completed all 112 courses in CHEM (saved 112 to database)     \n",
      "✅ Successfully scraped CHEM in 0.8 minutes\n",
      "\n",
      "📚 [25/152] Scraping CHER...\n",
      "   Found 5 courses to parse\n",
      "   ✓ Completed all 5 courses in CHER (saved 5 to database)     \n",
      "✅ Successfully scraped CHER in 0.0 minutes\n",
      "\n",
      "📚 [26/152] Scraping CHWA...\n",
      "   Found 3 courses to parse\n",
      "   ✓ Completed all 3 courses in CHWA (saved 3 to database)     \n",
      "✅ Successfully scraped CHWA in 0.0 minutes\n",
      "\n",
      "📚 [27/152] Scraping CHIN...\n",
      "   Found 47 courses to parse\n",
      "   ✓ Completed all 47 courses in CHIN (saved 47 to database)     \n",
      "✅ Successfully scraped CHIN in 0.2 minutes\n",
      "\n",
      "📚 [28/152] Scraping PLAN...\n",
      "   Found 121 courses to parse\n",
      "   ✓ Completed all 121 courses in PLAN (saved 121 to database)     \n",
      "✅ Successfully scraped PLAN in 0.2 minutes\n",
      "\n",
      "📚 [29/152] Scraping CLAR...\n",
      "   Found 44 courses to parse\n",
      "   ✓ Completed all 44 courses in CLAR (saved 44 to database)     \n",
      "✅ Successfully scraped CLAR in 0.2 minutes\n",
      "\n",
      "📚 [30/152] Scraping CLAS...\n",
      "   Found 51 courses to parse\n",
      "   ✓ Completed all 51 courses in CLAS (saved 51 to database)     \n",
      "✅ Successfully scraped CLAS in 0.1 minutes\n",
      "\n",
      "📚 [31/152] Scraping CLSC...\n",
      "   Found 43 courses to parse\n",
      "   ✓ Completed all 43 courses in CLSC (saved 43 to database)     \n",
      "✅ Successfully scraped CLSC in 0.2 minutes\n",
      "\n",
      "📚 [32/152] Scraping CRMH...\n",
      "   Found 25 courses to parse\n",
      "   ✓ Completed all 25 courses in CRMH (saved 25 to database)     \n",
      "✅ Successfully scraped CRMH in 0.1 minutes\n",
      "\n",
      "📚 [33/152] Scraping COMM...\n",
      "   Found 213 courses to parse\n",
      "   ✓ Completed all 213 courses in COMM (saved 213 to database)     \n",
      "✅ Successfully scraped COMM in 0.8 minutes\n",
      "\n",
      "📚 [34/152] Scraping CMPL...\n",
      "   Found 138 courses to parse\n",
      "   ✓ Completed all 138 courses in CMPL (saved 138 to database)     \n",
      "✅ Successfully scraped CMPL in 0.2 minutes\n",
      "\n",
      "📚 [35/152] Scraping COMP...\n",
      "   Found 109 courses to parse\n",
      "   ✓ Completed all 109 courses in COMP (saved 109 to database)     \n",
      "✅ Successfully scraped COMP in 1.0 minutes\n",
      "\n",
      "📚 [36/152] Scraping EURO...\n",
      "   Found 20 courses to parse\n",
      "   ✓ Completed all 20 courses in EURO (saved 20 to database)     \n",
      "✅ Successfully scraped EURO in 0.0 minutes\n",
      "\n",
      "📚 [37/152] Scraping CZCH...\n",
      "   Found 10 courses to parse\n",
      "   ✓ Completed all 10 courses in CZCH (saved 10 to database)     \n",
      "✅ Successfully scraped CZCH in 0.1 minutes\n",
      "\n",
      "📚 [38/152] Scraping DATA...\n",
      "   Found 37 courses to parse\n",
      "   ✓ Completed all 37 courses in DATA (saved 37 to database)     \n",
      "✅ Successfully scraped DATA in 0.3 minutes\n",
      "\n",
      "📚 [39/152] Scraping DENG...\n",
      "   Found 11 courses to parse\n",
      "   ✓ Completed all 11 courses in DENG (saved 11 to database)     \n",
      "✅ Successfully scraped DENG in 0.0 minutes\n",
      "\n",
      "📚 [40/152] Scraping DHYG...\n",
      "   Found 54 courses to parse\n",
      "   ✓ Completed all 54 courses in DHYG (saved 54 to database)     \n",
      "✅ Successfully scraped DHYG in 0.2 minutes\n",
      "\n",
      "📚 [41/152] Scraping DHED...\n",
      "   Found 11 courses to parse\n",
      "   ✓ Completed all 11 courses in DHED (saved 11 to database)     \n",
      "✅ Successfully scraped DHED in 0.0 minutes\n",
      "\n",
      "📚 [42/152] Scraping DRAM...\n",
      "   Found 133 courses to parse\n",
      "   ✓ Completed all 133 courses in DRAM (saved 133 to database)     \n",
      "✅ Successfully scraped DRAM in 0.4 minutes\n",
      "\n",
      "📚 [43/152] Scraping DTCH...\n",
      "   Found 7 courses to parse\n",
      "   ✓ Completed all 7 courses in DTCH (saved 7 to database)     \n",
      "✅ Successfully scraped DTCH in 0.0 minutes\n",
      "\n",
      "📚 [44/152] Scraping EMES...\n",
      "   Found 133 courses to parse\n",
      "   ✓ Completed all 133 courses in EMES (saved 133 to database)     \n",
      "✅ Successfully scraped EMES in 0.7 minutes\n",
      "\n",
      "📚 [45/152] Scraping ECON...\n",
      "   Found 145 courses to parse\n",
      "   ✓ Completed all 145 courses in ECON (saved 145 to database)     \n",
      "✅ Successfully scraped ECON in 1.1 minutes\n",
      "\n",
      "📚 [46/152] Scraping EDUC...\n",
      "   Found 346 courses to parse\n",
      "   ✓ Completed all 346 courses in EDUC (saved 346 to database)     \n",
      "✅ Successfully scraped EDUC in 0.8 minutes\n",
      "\n",
      "📚 [47/152] Scraping ENDO...\n",
      "   Found 4 courses to parse\n",
      "   ✓ Completed all 4 courses in ENDO (saved 4 to database)     \n",
      "✅ Successfully scraped ENDO in 0.0 minutes\n",
      "\n",
      "📚 [48/152] Scraping ENGL...\n",
      "   Found 341 courses to parse\n",
      "   ✓ Completed all 341 courses in ENGL (saved 341 to database)     \n",
      "✅ Successfully scraped ENGL in 0.7 minutes\n",
      "\n",
      "📚 [49/152] Scraping ENVR...\n",
      "   Found 133 courses to parse\n",
      "   ✓ Completed all 133 courses in ENVR (saved 133 to database)     \n",
      "✅ Successfully scraped ENVR in 0.5 minutes\n",
      "\n",
      "📚 [50/152] Scraping ENEC...\n",
      "   Found 135 courses to parse\n",
      "   ✓ Completed all 135 courses in ENEC (saved 135 to database)     \n",
      "✅ Successfully scraped ENEC in 0.6 minutes\n",
      "\n",
      "📚 [51/152] Scraping EPID...\n",
      "   Found 73 courses to parse\n",
      "   ✓ Completed all 73 courses in EPID (saved 73 to database)     \n",
      "✅ Successfully scraped EPID in 0.4 minutes\n",
      "\n",
      "📚 [52/152] Scraping EXSS...\n",
      "   Found 112 courses to parse\n",
      "   ✓ Completed all 112 courses in EXSS (saved 112 to database)     \n",
      "✅ Successfully scraped EXSS in 0.6 minutes\n",
      "\n",
      "📚 [53/152] Scraping EDMX...\n",
      "   Found 45 courses to parse\n",
      "   ✓ Completed all 45 courses in EDMX (saved 45 to database)     \n",
      "✅ Successfully scraped EDMX in 0.1 minutes\n",
      "\n",
      "📚 [54/152] Scraping SPCL...\n",
      "   Found 3 courses to parse\n",
      "   ✓ Completed all 3 courses in SPCL (saved 3 to database)     \n",
      "✅ Successfully scraped SPCL in 0.0 minutes\n",
      "\n",
      "📚 [55/152] Scraping DPET...\n",
      "   Found 9 courses to parse\n",
      "   ✓ Completed all 9 courses in DPET (saved 9 to database)     \n",
      "✅ Successfully scraped DPET in 0.0 minutes\n",
      "\n",
      "📚 [56/152] Scraping FOLK...\n",
      "   Found 45 courses to parse\n",
      "   ✓ Completed all 45 courses in FOLK (saved 45 to database)     \n",
      "✅ Successfully scraped FOLK in 0.1 minutes\n",
      "\n",
      "📚 [57/152] Scraping FREN...\n",
      "   Found 107 courses to parse\n",
      "   ✓ Completed all 107 courses in FREN (saved 107 to database)     \n",
      "✅ Successfully scraped FREN in 1.1 minutes\n",
      "\n",
      "📚 [58/152] Scraping GNET...\n",
      "   Found 37 courses to parse\n",
      "   ✓ Completed all 37 courses in GNET (saved 37 to database)     \n",
      "✅ Successfully scraped GNET in 0.1 minutes\n",
      "\n",
      "📚 [59/152] Scraping GEOG...\n",
      "   Found 143 courses to parse\n",
      "   ✓ Completed all 143 courses in GEOG (saved 143 to database)     \n",
      "✅ Successfully scraped GEOG in 0.4 minutes\n",
      "\n",
      "📚 [60/152] Scraping GEOL...\n",
      "   Found 18 courses to parse\n",
      "   ✓ Completed all 18 courses in GEOL (saved 18 to database)     \n",
      "✅ Successfully scraped GEOL in 0.0 minutes\n",
      "\n",
      "📚 [61/152] Scraping GERM...\n",
      "   Found 130 courses to parse\n",
      "   ✓ Completed all 130 courses in GERM (saved 130 to database)     \n",
      "✅ Successfully scraped GERM in 0.5 minutes\n",
      "\n",
      "📚 [62/152] Scraping GSLL...\n",
      "   Found 68 courses to parse\n",
      "   ✓ Completed all 68 courses in GSLL (saved 68 to database)     \n",
      "✅ Successfully scraped GSLL in 0.1 minutes\n",
      "\n",
      "📚 [63/152] Scraping GLBL...\n",
      "   Found 51 courses to parse\n",
      "   ✓ Completed all 51 courses in GLBL (saved 51 to database)     \n",
      "✅ Successfully scraped GLBL in 0.1 minutes\n",
      "\n",
      "📚 [64/152] Scraping GOVT...\n",
      "   Found 4 courses to parse\n",
      "   ✓ Completed all 4 courses in GOVT (saved 4 to database)     \n",
      "✅ Successfully scraped GOVT in 0.0 minutes\n",
      "\n",
      "📚 [65/152] Scraping GRAD...\n",
      "   Found 27 courses to parse\n",
      "   ✓ Completed all 27 courses in GRAD (saved 27 to database)     \n",
      "✅ Successfully scraped GRAD in 0.1 minutes\n",
      "\n",
      "📚 [66/152] Scraping GREK...\n",
      "   Found 34 courses to parse\n",
      "   ✓ Completed all 34 courses in GREK (saved 34 to database)     \n",
      "✅ Successfully scraped GREK in 0.1 minutes\n",
      "\n",
      "📚 [67/152] Scraping HBEH...\n",
      "   Found 74 courses to parse\n",
      "   ✓ Completed all 74 courses in HBEH (saved 74 to database)     \n",
      "✅ Successfully scraped HBEH in 0.2 minutes\n",
      "\n",
      "📚 [68/152] Scraping HPM...\n",
      "   Found 132 courses to parse\n",
      "   ✓ Completed all 132 courses in HPM (saved 132 to database)     \n",
      "✅ Successfully scraped HPM in 0.4 minutes\n",
      "\n",
      "📚 [69/152] Scraping HEBR...\n",
      "   Found 6 courses to parse\n",
      "   ✓ Completed all 6 courses in HEBR (saved 6 to database)     \n",
      "✅ Successfully scraped HEBR in 0.1 minutes\n",
      "\n",
      "📚 [70/152] Scraping HNUR...\n",
      "   Found 17 courses to parse\n",
      "   ✓ Completed all 17 courses in HNUR (saved 17 to database)     \n",
      "✅ Successfully scraped HNUR in 0.1 minutes\n",
      "\n",
      "📚 [71/152] Scraping HIST...\n",
      "   Found 416 courses to parse\n",
      "   ✓ Completed all 416 courses in HIST (saved 416 to database)     \n",
      "✅ Successfully scraped HIST in 0.6 minutes\n",
      "\n",
      "📚 [72/152] Scraping HUNG...\n",
      "   Found 9 courses to parse\n",
      "   ✓ Completed all 9 courses in HUNG (saved 9 to database)     \n",
      "✅ Successfully scraped HUNG in 0.0 minutes\n",
      "\n",
      "📚 [73/152] Scraping INLS...\n",
      "   Found 129 courses to parse\n",
      "   ✓ Completed all 129 courses in INLS (saved 129 to database)     \n",
      "✅ Successfully scraped INLS in 0.5 minutes\n",
      "\n",
      "📚 [74/152] Scraping IDST...\n",
      "   Found 67 courses to parse\n",
      "   ✓ Completed all 67 courses in IDST (saved 67 to database)     \n",
      "✅ Successfully scraped IDST in 0.3 minutes\n",
      "\n",
      "📚 [75/152] Scraping ICMU...\n",
      "   Found 3 courses to parse\n",
      "   ✓ Completed all 3 courses in ICMU (saved 3 to database)     \n",
      "✅ Successfully scraped ICMU in 0.0 minutes\n",
      "\n",
      "📚 [76/152] Scraping ITAL...\n",
      "   Found 61 courses to parse\n",
      "   ✓ Completed all 61 courses in ITAL (saved 61 to database)     \n",
      "✅ Successfully scraped ITAL in 0.3 minutes\n",
      "\n",
      "📚 [77/152] Scraping JAPN...\n",
      "   Found 29 courses to parse\n",
      "   ✓ Completed all 29 courses in JAPN (saved 29 to database)     \n",
      "✅ Successfully scraped JAPN in 0.2 minutes\n",
      "\n",
      "📚 [78/152] Scraping JWST...\n",
      "   Found 68 courses to parse\n",
      "   ✓ Completed all 68 courses in JWST (saved 68 to database)     \n",
      "✅ Successfully scraped JWST in 0.2 minutes\n",
      "\n",
      "📚 [79/152] Scraping SWAH...\n",
      "   Found 9 courses to parse\n",
      "   ✓ Completed all 9 courses in SWAH (saved 9 to database)     \n",
      "✅ Successfully scraped SWAH in 0.1 minutes\n",
      "\n",
      "📚 [80/152] Scraping KOR...\n",
      "   Found 20 courses to parse\n",
      "   ✓ Completed all 20 courses in KOR (saved 20 to database)     \n",
      "✅ Successfully scraped KOR in 0.1 minutes\n",
      "\n",
      "📚 [81/152] Scraping LTAM...\n",
      "   Found 24 courses to parse\n",
      "   ✓ Completed all 24 courses in LTAM (saved 24 to database)     \n",
      "✅ Successfully scraped LTAM in 0.1 minutes\n",
      "\n",
      "📚 [82/152] Scraping LATN...\n",
      "   Found 53 courses to parse\n",
      "   ✓ Completed all 53 courses in LATN (saved 53 to database)     \n",
      "✅ Successfully scraped LATN in 0.6 minutes\n",
      "\n",
      "📚 [83/152] Scraping LAW...\n",
      "   Found 204 courses to parse\n",
      "   ✓ Completed all 204 courses in LAW (saved 204 to database)     \n",
      "✅ Successfully scraped LAW in 0.6 minutes\n",
      "\n",
      "📚 [84/152] Scraping LFIT...\n",
      "   Found 24 courses to parse\n",
      "   ✓ Completed all 24 courses in LFIT (saved 24 to database)     \n",
      "✅ Successfully scraped LFIT in 0.0 minutes\n",
      "\n",
      "📚 [85/152] Scraping LGLA...\n",
      "   Found 4 courses to parse\n",
      "   ✓ Completed all 4 courses in LGLA (saved 4 to database)     \n",
      "✅ Successfully scraped LGLA in 0.0 minutes\n",
      "\n",
      "📚 [86/152] Scraping LING...\n",
      "   Found 97 courses to parse\n",
      "   ✓ Completed all 97 courses in LING (saved 97 to database)     \n",
      "✅ Successfully scraped LING in 0.5 minutes\n",
      "\n",
      "📚 [87/152] Scraping MACD...\n",
      "   Found 6 courses to parse\n",
      "   ✓ Completed all 6 courses in MACD (saved 6 to database)     \n",
      "✅ Successfully scraped MACD in 0.0 minutes\n",
      "\n",
      "📚 [88/152] Scraping MNGT...\n",
      "   Found 16 courses to parse\n",
      "   ✓ Completed all 16 courses in MNGT (saved 16 to database)     \n",
      "✅ Successfully scraped MNGT in 0.1 minutes\n",
      "\n",
      "📚 [89/152] Scraping MASC...\n",
      "   Found 2 courses to parse\n",
      "   ✓ Completed all 2 courses in MASC (saved 2 to database)     \n",
      "✅ Successfully scraped MASC in 0.0 minutes\n",
      "\n",
      "📚 [90/152] Scraping MTSC...\n",
      "   Found 34 courses to parse\n",
      "   ✓ Completed all 34 courses in MTSC (saved 34 to database)     \n",
      "✅ Successfully scraped MTSC in 0.1 minutes\n",
      "\n",
      "📚 [91/152] Scraping MHCH...\n",
      "   Found 45 courses to parse\n",
      "   ✓ Completed all 45 courses in MHCH (saved 45 to database)     \n",
      "✅ Successfully scraped MHCH in 0.1 minutes\n",
      "\n",
      "📚 [92/152] Scraping MATH...\n",
      "   Found 110 courses to parse\n",
      "   ✓ Completed all 110 courses in MATH (saved 110 to database)     \n",
      "✅ Successfully scraped MATH in 0.7 minutes\n",
      "\n",
      "📚 [93/152] Scraping MEJO...\n",
      "   Found 191 courses to parse\n",
      "   ✓ Completed all 191 courses in MEJO (saved 191 to database)     \n",
      "✅ Successfully scraped MEJO in 0.6 minutes\n",
      "\n",
      "📚 [94/152] Scraping MCRO...\n",
      "   Found 26 courses to parse\n",
      "   ✓ Completed all 26 courses in MCRO (saved 26 to database)     \n",
      "✅ Successfully scraped MCRO in 0.1 minutes\n",
      "\n",
      "📚 [95/152] Scraping MUSC...\n",
      "   Found 161 courses to parse\n",
      "   ✓ Completed all 161 courses in MUSC (saved 161 to database)     \n",
      "✅ Successfully scraped MUSC in 0.6 minutes\n",
      "\n",
      "📚 [96/152] Scraping NAVS...\n",
      "   Found 15 courses to parse\n",
      "   ✓ Completed all 15 courses in NAVS (saved 15 to database)     \n",
      "✅ Successfully scraped NAVS in 0.0 minutes\n",
      "\n",
      "📚 [97/152] Scraping NDSS...\n",
      "   Found 17 courses to parse\n",
      "   ✓ Completed all 17 courses in NDSS (saved 17 to database)     \n",
      "✅ Successfully scraped NDSS in 0.1 minutes\n",
      "\n",
      "📚 [98/152] Scraping NBIO...\n",
      "   Found 37 courses to parse\n",
      "   ✓ Completed all 37 courses in NBIO (saved 37 to database)     \n",
      "✅ Successfully scraped NBIO in 0.1 minutes\n",
      "\n",
      "📚 [99/152] Scraping NSCI...\n",
      "   Found 41 courses to parse\n",
      "   ✓ Completed all 41 courses in NSCI (saved 41 to database)     \n",
      "✅ Successfully scraped NSCI in 0.3 minutes\n",
      "\n",
      "📚 [100/152] Scraping NURS...\n",
      "   Found 169 courses to parse\n",
      "   ✓ Completed all 169 courses in NURS (saved 169 to database)     \n",
      "✅ Successfully scraped NURS in 1.1 minutes\n",
      "\n",
      "📚 [101/152] Scraping NUTR...\n",
      "   Found 59 courses to parse\n",
      "   ✓ Completed all 59 courses in NUTR (saved 59 to database)     \n",
      "✅ Successfully scraped NUTR in 0.3 minutes\n",
      "\n",
      "📚 [102/152] Scraping OCSC...\n",
      "   Found 5 courses to parse\n",
      "   ✓ Completed all 5 courses in OCSC (saved 5 to database)     \n",
      "✅ Successfully scraped OCSC in 0.0 minutes\n",
      "\n",
      "📚 [103/152] Scraping OCCT...\n",
      "   Found 25 courses to parse\n",
      "   ✓ Completed all 25 courses in OCCT (saved 25 to database)     \n",
      "✅ Successfully scraped OCCT in 0.1 minutes\n",
      "\n",
      "📚 [104/152] Scraping OPER...\n",
      "   Found 7 courses to parse\n",
      "   ✓ Completed all 7 courses in OPER (saved 7 to database)     \n",
      "✅ Successfully scraped OPER in 0.0 minutes\n",
      "\n",
      "📚 [105/152] Scraping ORPA...\n",
      "   Found 6 courses to parse\n",
      "   ✓ Completed all 6 courses in ORPA (saved 6 to database)     \n",
      "✅ Successfully scraped ORPA in 0.0 minutes\n",
      "\n",
      "📚 [106/152] Scraping ORAD...\n",
      "   Found 8 courses to parse\n",
      "   ✓ Completed all 8 courses in ORAD (saved 8 to database)     \n",
      "✅ Successfully scraped ORAD in 0.0 minutes\n",
      "\n",
      "📚 [107/152] Scraping ORTH...\n",
      "   Found 12 courses to parse\n",
      "   ✓ Completed all 12 courses in ORTH (saved 12 to database)     \n",
      "✅ Successfully scraped ORTH in 0.0 minutes\n",
      "\n",
      "📚 [108/152] Scraping PATH...\n",
      "   Found 22 courses to parse\n",
      "   ✓ Completed all 22 courses in PATH (saved 22 to database)     \n",
      "✅ Successfully scraped PATH in 0.1 minutes\n",
      "\n",
      "📚 [109/152] Scraping PWAD...\n",
      "   Found 112 courses to parse\n",
      "   ✓ Completed all 112 courses in PWAD (saved 112 to database)     \n",
      "✅ Successfully scraped PWAD in 0.2 minutes\n",
      "\n",
      "📚 [110/152] Scraping PEDO...\n",
      "   Found 7 courses to parse\n",
      "   ✓ Completed all 7 courses in PEDO (saved 7 to database)     \n",
      "✅ Successfully scraped PEDO in 0.0 minutes\n",
      "\n",
      "📚 [111/152] Scraping PERI...\n",
      "   Found 9 courses to parse\n",
      "   ✓ Completed all 9 courses in PERI (saved 9 to database)     \n",
      "✅ Successfully scraped PERI in 0.0 minutes\n",
      "\n",
      "📚 [112/152] Scraping PRSN...\n",
      "   Found 6 courses to parse\n",
      "   ✓ Completed all 6 courses in PRSN (saved 6 to database)     \n",
      "✅ Successfully scraped PRSN in 0.1 minutes\n",
      "\n",
      "📚 [113/152] Scraping PHRS...\n",
      "   Found 38 courses to parse\n",
      "   ✓ Completed all 38 courses in PHRS (saved 38 to database)     \n",
      "✅ Successfully scraped PHRS in 0.2 minutes\n",
      "\n",
      "📚 [114/152] Scraping DPMP...\n",
      "   Found 6 courses to parse\n",
      "   ✓ Completed all 6 courses in DPMP (saved 6 to database)     \n",
      "✅ Successfully scraped DPMP in 0.0 minutes\n",
      "\n",
      "📚 [115/152] Scraping PHCO...\n",
      "   Found 38 courses to parse\n",
      "   ✓ Completed all 38 courses in PHCO (saved 38 to database)     \n",
      "✅ Successfully scraped PHCO in 0.1 minutes\n",
      "\n",
      "📚 [116/152] Scraping PHCY...\n",
      "   Found 112 courses to parse\n",
      "   ✓ Completed all 112 courses in PHCY (saved 112 to database)     \n",
      "✅ Successfully scraped PHCY in 0.8 minutes\n",
      "\n",
      "📚 [117/152] Scraping DPOP...\n",
      "   Found 9 courses to parse\n",
      "   ✓ Completed all 9 courses in DPOP (saved 9 to database)     \n",
      "✅ Successfully scraped DPOP in 0.0 minutes\n",
      "\n",
      "📚 [118/152] Scraping PHIL...\n",
      "   Found 183 courses to parse\n",
      "   ✓ Completed all 183 courses in PHIL (saved 183 to database)     \n",
      "✅ Successfully scraped PHIL in 0.7 minutes\n",
      "\n",
      "📚 [119/152] Scraping PHYA...\n",
      "   Found 43 courses to parse\n",
      "   ✓ Completed all 43 courses in PHYA (saved 43 to database)     \n",
      "✅ Successfully scraped PHYA in 0.1 minutes\n",
      "\n",
      "📚 [120/152] Scraping PASC...\n",
      "   Found 36 courses to parse\n",
      "   ✓ Completed all 36 courses in PASC (saved 36 to database)     \n",
      "✅ Successfully scraped PASC in 0.1 minutes\n",
      "\n",
      "📚 [121/152] Scraping PHYS...\n",
      "   Found 100 courses to parse\n",
      "   ✓ Completed all 100 courses in PHYS (saved 100 to database)     \n",
      "✅ Successfully scraped PHYS in 0.8 minutes\n",
      "\n",
      "📚 [122/152] Scraping PLSH...\n",
      "   Found 10 courses to parse\n",
      "   ✓ Completed all 10 courses in PLSH (saved 10 to database)     \n",
      "✅ Successfully scraped PLSH in 0.1 minutes\n",
      "\n",
      "📚 [123/152] Scraping POLI...\n",
      "   Found 242 courses to parse\n",
      "   ✓ Completed all 242 courses in POLI (saved 242 to database)     \n",
      "✅ Successfully scraped POLI in 0.5 minutes\n",
      "\n",
      "📚 [124/152] Scraping PORT...\n",
      "   Found 51 courses to parse\n",
      "   ✓ Completed all 51 courses in PORT (saved 51 to database)     \n",
      "✅ Successfully scraped PORT in 0.2 minutes\n",
      "\n",
      "📚 [125/152] Scraping PACE...\n",
      "   Found 7 courses to parse\n",
      "   ✓ Completed all 7 courses in PACE (saved 7 to database)     \n",
      "✅ Successfully scraped PACE in 0.0 minutes\n",
      "\n",
      "📚 [126/152] Scraping PROS...\n",
      "   Found 9 courses to parse\n",
      "   ✓ Completed all 9 courses in PROS (saved 9 to database)     \n",
      "✅ Successfully scraped PROS in 0.0 minutes\n",
      "\n",
      "📚 [127/152] Scraping PSYC...\n",
      "   Found 183 courses to parse\n",
      "   ✓ Completed all 183 courses in PSYC (saved 183 to database)     \n",
      "✅ Successfully scraped PSYC in 1.0 minutes\n",
      "\n",
      "📚 [128/152] Scraping PUBA...\n",
      "   Found 51 courses to parse\n",
      "   ✓ Completed all 51 courses in PUBA (saved 51 to database)     \n",
      "✅ Successfully scraped PUBA in 0.2 minutes\n",
      "\n",
      "📚 [129/152] Scraping PUBH...\n",
      "   Found 59 courses to parse\n",
      "   ✓ Completed all 59 courses in PUBH (saved 59 to database)     \n",
      "✅ Successfully scraped PUBH in 0.2 minutes\n",
      "\n",
      "📚 [130/152] Scraping PLCY...\n",
      "   Found 122 courses to parse\n",
      "   ✓ Completed all 122 courses in PLCY (saved 122 to database)     \n",
      "✅ Successfully scraped PLCY in 0.3 minutes\n",
      "\n",
      "📚 [131/152] Scraping RADI...\n",
      "   Found 31 courses to parse\n",
      "   ✓ Completed all 31 courses in RADI (saved 31 to database)     \n",
      "✅ Successfully scraped RADI in 0.2 minutes\n",
      "\n",
      "📚 [132/152] Scraping RECR...\n",
      "   Found 13 courses to parse\n",
      "   ✓ Completed all 13 courses in RECR (saved 13 to database)     \n",
      "✅ Successfully scraped RECR in 0.0 minutes\n",
      "\n",
      "📚 [133/152] Scraping RELI...\n",
      "   Found 295 courses to parse\n",
      "   ✓ Completed all 295 courses in RELI (saved 295 to database)     \n",
      "✅ Successfully scraped RELI in 0.5 minutes\n",
      "\n",
      "📚 [134/152] Scraping ROML...\n",
      "   Found 49 courses to parse\n",
      "   ✓ Completed all 49 courses in ROML (saved 49 to database)     \n",
      "✅ Successfully scraped ROML in 0.1 minutes\n",
      "\n",
      "📚 [135/152] Scraping RUSS...\n",
      "   Found 33 courses to parse\n",
      "   ✓ Completed all 33 courses in RUSS (saved 33 to database)     \n",
      "✅ Successfully scraped RUSS in 0.1 minutes\n",
      "\n",
      "📚 [136/152] Scraping SCLL...\n",
      "   Found 32 courses to parse\n",
      "   ✓ Completed all 32 courses in SCLL (saved 32 to database)     \n",
      "✅ Successfully scraped SCLL in 0.1 minutes\n",
      "\n",
      "📚 [137/152] Scraping SPHG...\n",
      "   Found 23 courses to parse\n",
      "   ✓ Completed all 23 courses in SPHG (saved 23 to database)     \n",
      "✅ Successfully scraped SPHG in 0.1 minutes\n",
      "\n",
      "📚 [138/152] Scraping SLAV...\n",
      "   Found 11 courses to parse\n",
      "   ✓ Completed all 11 courses in SLAV (saved 11 to database)     \n",
      "✅ Successfully scraped SLAV in 0.0 minutes\n",
      "\n",
      "📚 [139/152] Scraping SOWO...\n",
      "   Found 108 courses to parse\n",
      "   ✓ Completed all 108 courses in SOWO (saved 108 to database)     \n",
      "✅ Successfully scraped SOWO in 0.6 minutes\n",
      "\n",
      "📚 [140/152] Scraping SOCI...\n",
      "   Found 134 courses to parse\n",
      "   ✓ Completed all 134 courses in SOCI (saved 134 to database)     \n",
      "✅ Successfully scraped SOCI in 0.3 minutes\n",
      "\n",
      "📚 [141/152] Scraping SPAN...\n",
      "   Found 119 courses to parse\n",
      "   ✓ Completed all 119 courses in SPAN (saved 119 to database)     \n",
      "✅ Successfully scraped SPAN in 0.7 minutes\n",
      "\n",
      "📚 [142/152] Scraping SPHS...\n",
      "   Found 108 courses to parse\n",
      "   ✓ Completed all 108 courses in SPHS (saved 108 to database)     \n",
      "✅ Successfully scraped SPHS in 0.4 minutes\n",
      "\n",
      "📚 [143/152] Scraping STOR...\n",
      "   Found 105 courses to parse\n",
      "   ✓ Completed all 105 courses in STOR (saved 105 to database)     \n",
      "✅ Successfully scraped STOR in 0.7 minutes\n",
      "\n",
      "📚 [144/152] Scraping ARTS...\n",
      "   Found 76 courses to parse\n",
      "   ✓ Completed all 76 courses in ARTS (saved 76 to database)     \n",
      "✅ Successfully scraped ARTS in 0.3 minutes\n",
      "\n",
      "📚 [145/152] Scraping TOXC...\n",
      "   Found 15 courses to parse\n",
      "   ✓ Completed all 15 courses in TOXC (saved 15 to database)     \n",
      "✅ Successfully scraped TOXC in 0.1 minutes\n",
      "\n",
      "📚 [146/152] Scraping TURK...\n",
      "   Found 6 courses to parse\n",
      "   ✓ Completed all 6 courses in TURK (saved 6 to database)     \n",
      "✅ Successfully scraped TURK in 0.1 minutes\n",
      "\n",
      "📚 [147/152] Scraping UKRN...\n",
      "   Found 2 courses to parse\n",
      "   ✓ Completed all 2 courses in UKRN (saved 2 to database)     \n",
      "✅ Successfully scraped UKRN in 0.0 minutes\n",
      "\n",
      "📚 [148/152] Scraping URES...\n",
      "   Found 3 courses to parse\n",
      "   ✓ Completed all 3 courses in URES (saved 3 to database)     \n",
      "✅ Successfully scraped URES in 0.0 minutes\n",
      "\n",
      "📚 [149/152] Scraping VIET...\n",
      "   Found 5 courses to parse\n",
      "   ✓ Completed all 5 courses in VIET (saved 5 to database)     \n",
      "✅ Successfully scraped VIET in 0.0 minutes\n",
      "\n",
      "📚 [150/152] Scraping WOLO...\n",
      "   Found 6 courses to parse\n",
      "   ✓ Completed all 6 courses in WOLO (saved 6 to database)     \n",
      "✅ Successfully scraped WOLO in 0.1 minutes\n",
      "\n",
      "📚 [151/152] Scraping WGST...\n",
      "   Found 159 courses to parse\n",
      "   ✓ Completed all 159 courses in WGST (saved 159 to database)     \n",
      "✅ Successfully scraped WGST in 0.4 minutes\n",
      "\n",
      "📚 [152/152] Scraping YORU...\n",
      "   Found 3 courses to parse\n",
      "   ✓ Completed all 3 courses in YORU (saved 3 to database)     \n",
      "✅ Successfully scraped YORU in 0.0 minutes\n",
      "\n",
      "⏱️  Total scraping time: 39.2 minutes\n",
      "\n",
      "📊 Statistics:\n",
      "   Total API calls: 2759\n",
      "   Failed parses: 6\n",
      "\n",
      "⚠️  Failed to parse requisites for:\n",
      "   - FREN 505: Expecting property name enclosed in double quotes:...\n",
      "   - FREN 555: Expecting property name enclosed in double quotes:...\n",
      "   - FREN 566: Expecting property name enclosed in double quotes:...\n",
      "   - FREN 611: Expecting property name enclosed in double quotes:...\n",
      "   - LATN 352: Expecting property name enclosed in double quotes:...\n",
      "   ... and 1 more\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main execution\n",
    "# Initialize components\n",
    "parser = RequisiteParser(model=\"gemini-2.5-flash-lite\", delay=0.5)\n",
    "db_manager = DatabaseManager(DATABASE_URL)\n",
    "\n",
    "# Configuration options\n",
    "MODE = 'database'  # 'database', 'json', or 'both'\n",
    "DRY_RUN = False    # Set to True to test without saving\n",
    "UPDATE_EXISTING = True  # Set to False to skip existing courses\n",
    "\n",
    "# Option 1: Scrape sample departments\n",
    "# sample_departments = {\"COMP\", \"BIOL\", \"CHEM\"}\n",
    "# courses = scrape_all_courses(\n",
    "#     parser, \n",
    "#     db_manager,\n",
    "#     only=sample_departments,\n",
    "#     mode=MODE,\n",
    "#     dry_run=DRY_RUN,\n",
    "#     update_existing=UPDATE_EXISTING\n",
    "# )\n",
    "\n",
    "# # Save JSON backup if requested\n",
    "# if MODE in ['json', 'both'] and courses:\n",
    "#     save_to_json(courses, \"unc_courses_sample.json\")\n",
    "\n",
    "# Option 2: Scrape all departments\n",
    "courses = scrape_all_courses(parser, db_manager, mode=MODE)\n",
    "if MODE in ['json', 'both'] and courses:\n",
    "    save_to_json(courses, \"unc_courses_all.json\")\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\n📊 Statistics:\")\n",
    "print(f\"   Total API calls: {parser.api_calls}\")\n",
    "print(f\"   Failed parses: {len(parser.failed_parses)}\")\n",
    "if parser.failed_parses:\n",
    "    print(\"\\n⚠️  Failed to parse requisites for:\")\n",
    "    for course_id, error in parser.failed_parses[:5]:\n",
    "        print(f\"   - {course_id}: {error[:50]}...\")\n",
    "    if len(parser.failed_parses) > 5:\n",
    "        print(f\"   ... and {len(parser.failed_parses) - 5} more\")\n",
    "\n",
    "# Close database connection\n",
    "db_manager.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28faaee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Database verification\n",
    "def verify_scraping_results():\n",
    "    \"\"\"Verify what was scraped into the database.\"\"\"\n",
    "    from db_queries import CourseDatabase\n",
    "    \n",
    "    with CourseDatabase() as db:\n",
    "        stats = db.get_database_stats()\n",
    "        print(\"\\n🔍 Database Contents:\")\n",
    "        print(f\"   Total courses: {stats['total_courses']}\")\n",
    "        print(f\"   Courses with prerequisites: {stats['courses_with_prereqs']}\")\n",
    "        \n",
    "        # Show sample courses\n",
    "        print(\"\\n📚 Sample courses:\")\n",
    "        sample_courses = [\"COMP 110\", \"COMP 211\", \"BIOL 101\"]\n",
    "        for course_id in sample_courses:\n",
    "            course = db.get_course(course_id)\n",
    "            if course:\n",
    "                prereqs = db.get_course_prerequisites(course_id)\n",
    "                print(f\"   {course_id}: {course['name']}\")\n",
    "                if prereqs['prerequisites']:\n",
    "                    print(f\"      Prerequisites: {len(prereqs['prerequisites'])} groups\")\n",
    "\n",
    "# Run verification\n",
    "verify_scraping_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e465a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing 152 departments...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning departments: 100%|██████████| 152/152 [01:04<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Requisite Analysis Complete!\n",
      "\n",
      "Total courses across all departments: 10212\n",
      "Courses with requisites: 2759\n",
      "Courses without requisites: 7453\n",
      "Percentage with requisites: 27.0%\n",
      "\n",
      "💡 You will need 2759 API calls\n",
      "⏱️  Estimated time at 2.1s/call: 96.6 minutes\n",
      "\n",
      "📈 Top 10 departments by requisite count:\n",
      "   BIOL: 167/264 courses (63%)\n",
      "   PSYC: 101/183 courses (55%)\n",
      "   NURS: 91/169 courses (54%)\n",
      "   ECON: 81/145 courses (56%)\n",
      "   COMP: 79/109 courses (72%)\n",
      "   PHCY: 75/112 courses (67%)\n",
      "   CHEM: 74/112 courses (66%)\n",
      "   SPAN: 71/119 courses (60%)\n",
      "   MATH: 70/110 courses (64%)\n",
      "   COMM: 69/213 courses (32%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell to count non-empty requisites across all departments\n",
    "def count_requisites(only=None):\n",
    "    \"\"\"Count how many courses have non-empty requisites across departments.\"\"\"\n",
    "    department_links = get_department_links(only=only)\n",
    "    \n",
    "    total_courses = 0\n",
    "    courses_with_requisites = 0\n",
    "    dept_stats = {}\n",
    "    \n",
    "    print(f\"🔍 Analyzing {len(department_links)} departments...\\n\")\n",
    "    \n",
    "    for dept_code, url in tqdm(department_links, desc=\"Scanning departments\"):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        course_blocks = soup.find_all(\"div\", class_=\"courseblock\")\n",
    "        \n",
    "        dept_total = len(course_blocks)\n",
    "        dept_with_reqs = 0\n",
    "        \n",
    "        for block in course_blocks:\n",
    "            req_span = block.find(\"span\", class_=\"text detail-requisites margin--default\")\n",
    "            if req_span and req_span.text.strip() and req_span.text.strip() != \"Requisites:\":\n",
    "                dept_with_reqs += 1\n",
    "        \n",
    "        dept_stats[dept_code] = {\n",
    "            \"total\": dept_total,\n",
    "            \"with_requisites\": dept_with_reqs,\n",
    "            \"percentage\": (dept_with_reqs / dept_total * 100) if dept_total > 0 else 0\n",
    "        }\n",
    "        \n",
    "        total_courses += dept_total\n",
    "        courses_with_requisites += dept_with_reqs\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n📊 Requisite Analysis Complete!\\n\")\n",
    "    print(f\"Total courses across all departments: {total_courses}\")\n",
    "    print(f\"Courses with requisites: {courses_with_requisites}\")\n",
    "    print(f\"Courses without requisites: {total_courses - courses_with_requisites}\")\n",
    "    print(f\"Percentage with requisites: {courses_with_requisites/total_courses*100:.1f}%\")\n",
    "    print(f\"\\n💡 You will need {courses_with_requisites} API calls\")\n",
    "    print(f\"⏱️  Estimated time at 2.1s/call: {courses_with_requisites * 2.1 / 60:.1f} minutes\")\n",
    "    \n",
    "    # Show top departments by requisite count\n",
    "    print(f\"\\n📈 Top 10 departments by requisite count:\")\n",
    "    sorted_depts = sorted(dept_stats.items(), key=lambda x: x[1]['with_requisites'], reverse=True)[:10]\n",
    "    for dept, stats in sorted_depts:\n",
    "        print(f\"   {dept}: {stats['with_requisites']}/{stats['total']} courses ({stats['percentage']:.0f}%)\")\n",
    "    \n",
    "    return dept_stats\n",
    "\n",
    "# Run the analysis\n",
    "# For all departments:\n",
    "dept_stats = count_requisites()\n",
    "\n",
    "# Or for specific departments:\n",
    "# dept_stats = count_requisites(only={\"COMP\", \"MATH\", \"BIOL\", \"CHEM\", \"PHYS\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
